{"version":"1","data":{"project":{"id":"ODBjYmQ2MDg","name":"NLP SSM Final Project"},"document":{"id":"03479859-6510-4e2f-ae2a-bc66465290c8","name":"resume_5185_d2.txt"},"kinds":["TOKEN_BASED"],"rows":[[{"content":"Azure VM Admin, ETL Specialist Azure VM Admin, ETL Specialist Azure VM Admin, ETL Specialist - Analytikus Work Experience Azure VM Admin, ETL Specialist Analytikus - Miami, FL August 2015 to Present Created and administrated Azure virtual machine.","tokens":["Azure","VM","Admin,","ETL","Specialist","Azure","VM","Admin,","ETL","Specialist","Azure","VM","Admin,","ETL","Specialist","-","Analytikus","Work","Experience","Azure","VM","Admin,","ETL","Specialist","Analytikus","-","Miami,","FL","August","2015","to","Present","Created","and","administrated","Azure","virtual","machine."],"metadata":[]}],[{"content":"Federated Active directory with Azure Active Directory (Office 365).","tokens":["Federated","Active","directory","with","Azure","Active","Directory","(Office","365)."],"metadata":[]}],[{"content":"Provided password synchronization AD to Azure AD.","tokens":["Provided","password","synchronization","AD","to","Azure","AD."],"metadata":[]}],[{"content":"Configured SSRS Portal to share reports between SQL server accounts.","tokens":["Configured","SSRS","Portal","to","share","reports","between","SQL","server","accounts."],"metadata":[]}],[{"content":"Provided database administration (Users and permissions).","tokens":["Provided","database","administration","(Users","and","permissions)."],"metadata":[]}],[{"content":"Generated data encryption process.","tokens":["Generated","data","encryption","process."],"metadata":[]}],[{"content":"Configured Gateway and VPN in Azure VM.","tokens":["Configured","Gateway","and","VPN","in","Azure","VM."],"metadata":[]}],[{"content":"Provided SharePoint domain configuration with Office 365.","tokens":["Provided","SharePoint","domain","configuration","with","Office","365."],"metadata":[]}],[{"content":"Designed and implemented SSIS processes.","tokens":["Designed","and","implemented","SSIS","processes."],"metadata":[]}],[{"content":"Designed SSAS cubes.","tokens":["Designed","SSAS","cubes."],"metadata":[]}],[{"content":"Provided data modeling.","tokens":["Provided","data","modeling."],"metadata":[]}],[{"content":"Designed roles in SSAS to manage row-level-security using Active Directory.","tokens":["Designed","roles","in","SSAS","to","manage","row-level-security","using","Active","Directory."],"metadata":[]}],[{"content":"Installed and configured PowerBI installation and PowerBI Gateways in Azure VM.","tokens":["Installed","and","configured","PowerBI","installation","and","PowerBI","Gateways","in","Azure","VM."],"metadata":[]}],[{"content":"Managed Power BI portal sharing dashboards between Office 365 accounts.","tokens":["Managed","Power","BI","portal","sharing","dashboards","between","Office","365","accounts."],"metadata":[]}],[{"content":"Designed and implemented strategical dashboards in Power BI and SSRS.","tokens":["Designed","and","implemented","strategical","dashboards","in","Power","BI","and","SSRS."],"metadata":[]}],[{"content":"Manage PowerBI row-level-security Manage PowerBI service creating workspaces, sharing dashboards, scheduling datasets updates, and giving user permissions Created sheets and dashboards in Tableau.","tokens":["Manage","PowerBI","row-level-security","Manage","PowerBI","service","creating","workspaces,","sharing","dashboards,","scheduling","datasets","updates,","and","giving","user","permissions","Created","sheets","and","dashboards","in","Tableau."],"metadata":[]}],[{"content":"Shared and published dashboard.","tokens":["Shared","and","published","dashboard."],"metadata":[]}],[{"content":"Exported sheets graphics as PDF and images.","tokens":["Exported","sheets","graphics","as","PDF","and","images."],"metadata":[]}],[{"content":"Used databases and csv files as sources in Tableau.","tokens":["Used","databases","and","csv","files","as","sources","in","Tableau."],"metadata":[]}],[{"content":"Created KPIs using Tableau components to use in graphics.","tokens":["Created","KPIs","using","Tableau","components","to","use","in","graphics."],"metadata":[]}],[{"content":"Used Tableau Dashboard formatting.","tokens":["Used","Tableau","Dashboard","formatting."],"metadata":[]}],[{"content":"Used Tableau Dashboard designer for some devices as phone and tablet.","tokens":["Used","Tableau","Dashboard","designer","for","some","devices","as","phone","and","tablet."],"metadata":[]}],[{"content":"Created aggregated calculations in PowerBI using DAX programming.","tokens":["Created","aggregated","calculations","in","PowerBI","using","DAX","programming."],"metadata":[]}],[{"content":"I led teams of developers Created project timelines Reviewed legacy Informatica ETL to migrate to SSIS Informatica ETL processes migration to SSIS Created ETL processes in Azure Databricks Created Python scripts to extract transform and load data Configured Data Factory in Azure Created Data Factory process to run SSIS ETL processes in Azure Technologies and tools used: Azure Virtual machine, Office online services, SSIS, SSAS, SSRS, Power BI, Azure Active Directory, SharePoint, Office 365, Tableau Desktop, Tableau server, Azure Data Factory, Azure machine learning, Informatica PowerCenter 9.1 Front- end and ETL Developer Praxis - Mexico City, MX June 2014 to July 2015 Provided BRD files analysis and technical sheets for construction of the SSI system (extractions and transformations).","tokens":["I","led","teams","of","developers","Created","project","timelines","Reviewed","legacy","Informatica","ETL","to","migrate","to","SSIS","Informatica","ETL","processes","migration","to","SSIS","Created","ETL","processes","in","Azure","Databricks","Created","Python","scripts","to","extract","transform","and","load","data","Configured","Data","Factory","in","Azure","Created","Data","Factory","process","to","run","SSIS","ETL","processes","in","Azure","Technologies","and","tools","used:","Azure","Virtual","machine,","Office","online","services,","SSIS,","SSAS,","SSRS,","Power","BI,","Azure","Active","Directory,","SharePoint,","Office","365,","Tableau","Desktop,","Tableau","server,","Azure","Data","Factory,","Azure","machine","learning,","Informatica","PowerCenter","9.1","Front-","end","and","ETL","Developer","Praxis","-","Mexico","City,","MX","June","2014","to","July","2015","Provided","BRD","files","analysis","and","technical","sheets","for","construction","of","the","SSI","system","(extractions","and","transformations)."],"metadata":[]}],[{"content":"Provided database model analysis used in the DB2 database.","tokens":["Provided","database","model","analysis","used","in","the","DB2","database."],"metadata":[]}],[{"content":"Constructed queries in the DB2 database from Oracle legacy tables, temporary tables, work tables and check digits.","tokens":["Constructed","queries","in","the","DB2","database","from","Oracle","legacy","tables,","temporary","tables,","work","tables","and","check","digits."],"metadata":[]}],[{"content":"Designed and developed ETL processes for SSI legacy in Informatica.","tokens":["Designed","and","developed","ETL","processes","for","SSI","legacy","in","Informatica."],"metadata":[]}],[{"content":"Programmed validation routines as mentioned in technical sheet.","tokens":["Programmed","validation","routines","as","mentioned","in","technical","sheet."],"metadata":[]}],[{"content":"Programmed extraction queries to expedite migration Oracle database to DB2.","tokens":["Programmed","extraction","queries","to","expedite","migration","Oracle","database","to","DB2."],"metadata":[]}],[{"content":"Programmed process to generate the current age for each user from the RFC.","tokens":["Programmed","process","to","generate","the","current","age","for","each","user","from","the","RFC."],"metadata":[]}],[{"content":"Programmed processes to generate check digits in the extraction of SSI system.","tokens":["Programmed","processes","to","generate","check","digits","in","the","extraction","of","SSI","system."],"metadata":[]}],[{"content":"Provided application process for validating previous month and current month in SSI transformation.","tokens":["Provided","application","process","for","validating","previous","month","and","current","month","in","SSI","transformation."],"metadata":[]}],[{"content":"Configured sessions in Informatica to execute the maps.","tokens":["Configured","sessions","in","Informatica","to","execute","the","maps."],"metadata":[]}],[{"content":"Created cubes of some tables.","tokens":["Created","cubes","of","some","tables."],"metadata":[]}],[{"content":"Built OLAP cubes to optimize queries using dimensions.","tokens":["Built","OLAP","cubes","to","optimize","queries","using","dimensions."],"metadata":[]}],[{"content":"Deployed multidimensional DB.","tokens":["Deployed","multidimensional","DB."],"metadata":[]}],[{"content":"Configuration parameter files used in the execution for SSI sessions.","tokens":["Configuration","parameter","files","used","in","the","execution","for","SSI","sessions."],"metadata":[]}],[{"content":"Validated of ETL processes with BI specialist.","tokens":["Validated","of","ETL","processes","with","BI","specialist."],"metadata":[]}],[{"content":"Validated output files with business specialist.","tokens":["Validated","output","files","with","business","specialist."],"metadata":[]}],[{"content":"Ran processes by shell in development and pre-production environments.","tokens":["Ran","processes","by","shell","in","development","and","pre-production","environments."],"metadata":[]}],[{"content":"Documentation: Technical design, functional design, Data model, test, Runbook Technologies and tools used: Secure Shell, shell script, FileZilla 3.5.3, Linux, SSIS, Informatica PowerCenter 9.1, Oracle 11g, DB2 ETL and Front- end Programmer Comisin Nacional de Seguridad - Mexico City, MX June 2013 to January 2014 Activities: Designed and developed ETL processes for data migration from SQL Server to Oracle, considering tables, indexes, store procedures, users, etc. Initial loaded to database Oracle using the dump process.","tokens":["Documentation:","Technical","design,","functional","design,","Data","model,","test,","Runbook","Technologies","and","tools","used:","Secure","Shell,","shell","script,","FileZilla","3.5.3,","Linux,","SSIS,","Informatica","PowerCenter","9.1,","Oracle","11g,","DB2","ETL","and","Front-","end","Programmer","Comisin","Nacional","de","Seguridad","-","Mexico","City,","MX","June","2013","to","January","2014","Activities:","Designed","and","developed","ETL","processes","for","data","migration","from","SQL","Server","to","Oracle,","considering","tables,","indexes,","store","procedures,","users,","etc.","Initial","loaded","to","database","Oracle","using","the","dump","process."],"metadata":[]}],[{"content":"Construction of queries for extraction using table joins, sources that were delegated.","tokens":["Construction","of","queries","for","extraction","using","table","joins,","sources","that","were","delegated."],"metadata":[]}],[{"content":"Construction of XML files for uploading information through a system of extraction and load to a repository.","tokens":["Construction","of","XML","files","for","uploading","information","through","a","system","of","extraction","and","load","to","a","repository."],"metadata":[]}],[{"content":"Analysis of the structure of the information to develop the process of deltas of Oracle and SQL Server sources.","tokens":["Analysis","of","the","structure","of","the","information","to","develop","the","process","of","deltas","of","Oracle","and","SQL","Server","sources."],"metadata":[]}],[{"content":"Documented processes.","tokens":["Documented","processes."],"metadata":[]}],[{"content":"Programmed process to generate reports in SSRS.","tokens":["Programmed","process","to","generate","reports","in","SSRS."],"metadata":[]}],[{"content":"Added a dataset report based on queries.","tokens":["Added","a","dataset","report","based","on","queries."],"metadata":[]}],[{"content":"Did PowerPivot data analysis.","tokens":["Did","PowerPivot","data","analysis."],"metadata":[]}],[{"content":"Technologies and tools used: IBM Data Stage InfoSphere, SQL Server 2008, Secure Shell, Shell Script, Linux, Oracle 11g, SQL Server Reporting Services (SSRS), SQL Server Management Studio 2008, PowerPivot ETL Programmer Servicio de Administracion Tributaria, SAT - Mexico City, MX March 2013 to June 2013 Reviewed the process of \"large taxpayers\" in SAT for issues like NEPE, ANNUAL, DYP, and DIME.","tokens":["Technologies","and","tools","used:","IBM","Data","Stage","InfoSphere,","SQL","Server","2008,","Secure","Shell,","Shell","Script,","Linux,","Oracle","11g,","SQL","Server","Reporting","Services","(SSRS),","SQL","Server","Management","Studio","2008,","PowerPivot","ETL","Programmer","Servicio","de","Administracion","Tributaria,","SAT","-","Mexico","City,","MX","March","2013","to","June","2013","Reviewed","the","process","of","\"large","taxpayers\"","in","SAT","for","issues","like","NEPE,","ANNUAL,","DYP,","and","DIME."],"metadata":[]}],[{"content":"Constructed routines in the process control tablet speeding up the step, due to the time frame allowed the company.","tokens":["Constructed","routines","in","the","process","control","tablet","speeding","up","the","step,","due","to","the","time","frame","allowed","the","company."],"metadata":[]}],[{"content":"Managed VOID analysis process.","tokens":["Managed","VOID","analysis","process."],"metadata":[]}],[{"content":"Provided process analysis of loading tables NEPE, ANUALES DIME, and DYP issues.","tokens":["Provided","process","analysis","of","loading","tables","NEPE,","ANUALES","DIME,","and","DYP","issues."],"metadata":[]}],[{"content":"Addressed parameters directly in the project environment variables, also the input and output files, all in accordance with the standards in the business handled.","tokens":["Addressed","parameters","directly","in","the","project","environment","variables,","also","the","input","and","output","files,","all","in","accordance","with","the","standards","in","the","business","handled."],"metadata":[]}],[{"content":"Changed the comparison process with new RFC.","tokens":["Changed","the","comparison","process","with","new","RFC."],"metadata":[]}],[{"content":"Created bar charts in SQL Server Reporting Services (SSRS).","tokens":["Created","bar","charts","in","SQL","Server","Reporting","Services","(SSRS)."],"metadata":[]}],[{"content":"Created dashboards of correlated column charts.","tokens":["Created","dashboards","of","correlated","column","charts."],"metadata":[]}],[{"content":"Created dashboards in Power BI using Power Query.","tokens":["Created","dashboards","in","Power","BI","using","Power","Query."],"metadata":[]}],[{"content":"Created dashboards of nested calculations with Power Query.","tokens":["Created","dashboards","of","nested","calculations","with","Power","Query."],"metadata":[]}],[{"content":"Technologies and tools used: IBM Data Stage InfoSphere, Power BI, SSRS, Informix, Linux, Shell script, FileZilla, SSH ETL Programmer Archysoft - Mexico City, MX May 2012 to February 2013 Reviewed and processed scheduling information extraction data sources for DB2 ACCEDER and SINDO systems.","tokens":["Technologies","and","tools","used:","IBM","Data","Stage","InfoSphere,","Power","BI,","SSRS,","Informix,","Linux,","Shell","script,","FileZilla,","SSH","ETL","Programmer","Archysoft","-","Mexico","City,","MX","May","2012","to","February","2013","Reviewed","and","processed","scheduling","information","extraction","data","sources","for","DB2","ACCEDER","and","SINDO","systems."],"metadata":[]}],[{"content":"Programmed 40 ETL jobs for data cleansing and over 25 functions were developed to implement the business rules defined by the user area.","tokens":["Programmed","40","ETL","jobs","for","data","cleansing","and","over","25","functions","were","developed","to","implement","the","business","rules","defined","by","the","user","area."],"metadata":[]}],[{"content":"For temporary and permanent storage of information, programmed SQL scripts for creating tables, indexes, and other elements that allows for holding and processing information.","tokens":["For","temporary","and","permanent","storage","of","information,","programmed","SQL","scripts","for","creating","tables,","indexes,","and","other","elements","that","allows","for","holding","and","processing","information."],"metadata":[]}],[{"content":"Performed unit testing and was integral to all tests constructed on Data Stage jobs as well as job functions Q Basic and sequences built by work team.","tokens":["Performed","unit","testing","and","was","integral","to","all","tests","constructed","on","Data","Stage","jobs","as","well","as","job","functions","Q","Basic","and","sequences","built","by","work","team."],"metadata":[]}],[{"content":"The scheduled ETL processes were performed to standardize and clean the information had not suitable quality for the purposes needed by the user.","tokens":["The","scheduled","ETL","processes","were","performed","to","standardize","and","clean","the","information","had","not","suitable","quality","for","the","purposes","needed","by","the","user."],"metadata":[]}],[{"content":"Developed a process to eliminate duplicate records, extracting flow records that were repeated.","tokens":["Developed","a","process","to","eliminate","duplicate","records,","extracting","flow","records","that","were","repeated."],"metadata":[]}],[{"content":"This was performed by using hash files and other tables directly from SQL.","tokens":["This","was","performed","by","using","hash","files","and","other","tables","directly","from","SQL."],"metadata":[]}],[{"content":"Programmed before and after functions to make validation digits, dates processing and consistency of information uploaded to the data warehouse IMSS systems.","tokens":["Programmed","before","and","after","functions","to","make","validation","digits,","dates","processing","and","consistency","of","information","uploaded","to","the","data","warehouse","IMSS","systems."],"metadata":[]}],[{"content":"Designed and programmed loading processes information and data cleansing for data marts of SINDO and ACCEDER the two main systems of IMSS.","tokens":["Designed","and","programmed","loading","processes","information","and","data","cleansing","for","data","marts","of","SINDO","and","ACCEDER","the","two","main","systems","of","IMSS."],"metadata":[]}],[{"content":"Create Extract/Load detail dashboard in SSIS Technologies and tools used: SQL Server Management Studio, SQL Server 2012, Microsoft Visual Studio, DB2, Linux, Shell, SQL Developer, IBM Data Studio, SQL Developer, FileZilla, SSH, SSAS, SSIS IBM DataStage Developer Arquitectura en Software Consulting Firm - Mexico City, MX October 2011 to April 2012 at IMSS (Mexican Social Security Institute) Data Analysis Provided data analysis, identification of special characters, and structure definitions files.","tokens":["Create","Extract/Load","detail","dashboard","in","SSIS","Technologies","and","tools","used:","SQL","Server","Management","Studio,","SQL","Server","2012,","Microsoft","Visual","Studio,","DB2,","Linux,","Shell,","SQL","Developer,","IBM","Data","Studio,","SQL","Developer,","FileZilla,","SSH,","SSAS,","SSIS","IBM","DataStage","Developer","Arquitectura","en","Software","Consulting","Firm","-","Mexico","City,","MX","October","2011","to","April","2012","at","IMSS","(Mexican","Social","Security","Institute)","Data","Analysis","Provided","data","analysis,","identification","of","special","characters,","and","structure","definitions","files."],"metadata":[]}],[{"content":"Prepared reports of frequencies for defining business rules.","tokens":["Prepared","reports","of","frequencies","for","defining","business","rules."],"metadata":[]}],[{"content":"Documented processes and information flows.","tokens":["Documented","processes","and","information","flows."],"metadata":[]}],[{"content":"Developed notes profiling tables analyzed.","tokens":["Developed","notes","profiling","tables","analyzed."],"metadata":[]}],[{"content":"Identified anomalies data field and source of information.","tokens":["Identified","anomalies","data","field","and","source","of","information."],"metadata":[]}],[{"content":"Data Cleaning Programmed and processed shell script (UNIX) for cleaning planes, identification and correction of special characters and inconsistent data files.","tokens":["Data","Cleaning","Programmed","and","processed","shell","script","(UNIX)","for","cleaning","planes,","identification","and","correction","of","special","characters","and","inconsistent","data","files."],"metadata":[]}],[{"content":"Designed and programmed processes for IBM DataStage, business rules application for standardization of information.","tokens":["Designed","and","programmed","processes","for","IBM","DataStage,","business","rules","application","for","standardization","of","information."],"metadata":[]}],[{"content":"Information Extraction Analyzed and designed programming process for the extraction of information contained in data sources DB2 and ORACLE 11 g. Provided SQL programming scripts to build tables, indexes and constraints.","tokens":["Information","Extraction","Analyzed","and","designed","programming","process","for","the","extraction","of","information","contained","in","data","sources","DB2","and","ORACLE","11","g.","Provided","SQL","programming","scripts","to","build","tables,","indexes","and","constraints."],"metadata":[]}],[{"content":"Data Loading Scheduled ETL (DataStage) processes for the initial loads databases cleaning and standardization information (DB2).","tokens":["Data","Loading","Scheduled","ETL","(DataStage)","processes","for","the","initial","loads","databases","cleaning","and","standardization","information","(DB2)."],"metadata":[]}],[{"content":"Developed ETL processes using DataStage to load information to the analysis database (DB2) Technologies and tools used: IBM DataStage InfoSphere, Oracle 10G y 11g, DB2 9.5 y 97, Linux, Shell, SQL, Developer, IBM Data Studio, SQL Server, FileZilla, SSH Education Engineering Information and Communication Technologies Universidad Tecnolgica del valle de Toluca","tokens":["Developed","ETL","processes","using","DataStage","to","load","information","to","the","analysis","database","(DB2)","Technologies","and","tools","used:","IBM","DataStage","InfoSphere,","Oracle","10G","y","11g,","DB2","9.5","y","97,","Linux,","Shell,","SQL,","Developer,","IBM","Data","Studio,","SQL","Server,","FileZilla,","SSH","Education","Engineering","Information","and","Communication","Technologies","Universidad","Tecnolgica","del","valle","de","Toluca"],"metadata":[]}]],"labelerInfo":{"id":6814,"email":"cwilhel8@jh.edu","displayName":"Chris Wilhelm"},"labelSets":[{"name":"Resume Labels","index":0,"labelItems":[{"id":"Kj5U6n9NUqTXuWcB3c_7L","labelName":"College"},{"id":"rj7TW0-ymuRo6f_asXJab","labelName":"Company Name"},{"id":"SJEkdao76hjwbb6Fh5SY6","labelName":"Job Title"},{"id":"BjO8MdvK2KI4E5iMEeY4B","labelName":"Graduation Year"},{"id":"eS2knCRgLegeRw8bcmSWA","labelName":"Skill"},{"id":"IeEoIKabMl51BJE0gX2YL","labelName":"Degree"},{"id":"1R0C50AAwFbEp2O0WmRv6","labelName":"Location"},{"id":"zQcc4ygexJ_A3Gb0NeRTk","labelName":"Years of Experience"},{"id":"52S_6moRfhjC-yXgIBeCp","labelName":"Date"},{"id":"KiLarWmwlU1E3-FfdG6Qg","labelName":"Project"},{"id":"dyxKKTAxW1z5ifjPj42BI","labelName":"Name"}]}],"spanLabels":[],"arrowLabels":[],"boundingBoxLabels":[],"timeLabels":[],"comments":[],"url":"datasaur://static/6814/03479859-6510-4e2f-ae2a-bc66465290c8.txt"}}
