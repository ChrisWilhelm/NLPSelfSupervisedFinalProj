[{"text":"Sr. Spark Developer Sr. Spark Developer Sr. Spark Developer - CenterPoint Energy 8 years of extensive hands-on experience in IT industry including 5+ years experience in deployment of Hadoop Ecosystems like MapReduce, Yarn, Sqoop, Flume, Pig, Hive, HBase, Cassandra, Zoo Keeper, Oozie, and Ambari, BigQuery, Big Table and 5+ years experience on Spark, Storm, Scala, Python.","entities":[{"text":"Sr. Spark Developer","type":"Job Title","start_idx":0,"end_idx":19},{"text":"Sr. Spark Developer","type":"Job Title","start_idx":20,"end_idx":39},{"text":"Sr. Spark Developer","type":"Job Title","start_idx":40,"end_idx":59},{"text":"CenterPoint Energy","type":"Company Name","start_idx":62,"end_idx":80},{"text":"8 years","type":"Years of Experience","start_idx":81,"end_idx":88},{"text":"IT","type":"Skill","start_idx":125,"end_idx":127},{"text":"Hadoop","type":"Skill","start_idx":184,"end_idx":190},{"text":"MapReduce,","type":"Skill","start_idx":207,"end_idx":217},{"text":"Yarn,","type":"Skill","start_idx":218,"end_idx":223},{"text":"Sqoop,","type":"Skill","start_idx":224,"end_idx":230},{"text":"Flume,","type":"Skill","start_idx":231,"end_idx":237},{"text":"Pig,","type":"Skill","start_idx":238,"end_idx":242},{"text":"Hive,","type":"Skill","start_idx":243,"end_idx":248},{"text":"HBase,","type":"Skill","start_idx":249,"end_idx":255},{"text":"Cassandra,","type":"Skill","start_idx":256,"end_idx":266},{"text":"Zoo Keeper,","type":"Skill","start_idx":267,"end_idx":278},{"text":"Oozie,","type":"Skill","start_idx":279,"end_idx":285},{"text":"Ambari,","type":"Skill","start_idx":290,"end_idx":297},{"text":"BigQuery,","type":"Skill","start_idx":298,"end_idx":307},{"text":"Big Table","type":"Skill","start_idx":308,"end_idx":317},{"text":"Spark,","type":"Skill","start_idx":345,"end_idx":351},{"text":"Storm,","type":"Skill","start_idx":352,"end_idx":358},{"text":"Scala,","type":"Skill","start_idx":359,"end_idx":365},{"text":"Python.","type":"Skill","start_idx":366,"end_idx":373}]},{"text":"Experience in OLTP and OLAP design, development, testing, implementation and support of enterprise Data warehouses.","entities":[{"text":"OLTP","type":"Skill","start_idx":14,"end_idx":18},{"text":"OLAP","type":"Skill","start_idx":23,"end_idx":27},{"text":"development,","type":"Skill","start_idx":36,"end_idx":48},{"text":"testing,","type":"Skill","start_idx":49,"end_idx":57},{"text":"implementation","type":"Skill","start_idx":58,"end_idx":72},{"text":"support","type":"Skill","start_idx":77,"end_idx":84},{"text":"Data warehouses.","type":"Skill","start_idx":99,"end_idx":115}]},{"text":"Strong Knowledge in Hadoop Cluster Capacity Planning, Performance Tuning, Cluster Monitoring Extensive experience in business data science project life cycle including Data Acquisition, Data Cleaning, Data Manipulation, Data Validation, Data Mining, Machine Learning Algorithms, and Visualization Good Hands on experience in working with Ecosystems like Hive, Pig, Sqoop, Map Reduce, Flume, Oozie.","entities":[{"text":"Hadoop Cluster Capacity Planning,","type":"Skill","start_idx":20,"end_idx":53},{"text":"Performance Tuning,","type":"Skill","start_idx":54,"end_idx":73},{"text":"Cluster Monitoring","type":"Skill","start_idx":74,"end_idx":92},{"text":"business data science","type":"Skill","start_idx":117,"end_idx":138},{"text":"project life cycle","type":"Skill","start_idx":139,"end_idx":157},{"text":"Data Acquisition,","type":"Skill","start_idx":168,"end_idx":185},{"text":"Data Cleaning,","type":"Skill","start_idx":186,"end_idx":200},{"text":"Data Manipulation,","type":"Skill","start_idx":201,"end_idx":219},{"text":"Data Validation,","type":"Skill","start_idx":220,"end_idx":236},{"text":"Data Mining,","type":"Skill","start_idx":237,"end_idx":249},{"text":"Machine Learning Algorithms,","type":"Skill","start_idx":250,"end_idx":278},{"text":"Visualization","type":"Skill","start_idx":283,"end_idx":296},{"text":"Hive,","type":"Skill","start_idx":354,"end_idx":359},{"text":"Pig,","type":"Skill","start_idx":360,"end_idx":364},{"text":"Sqoop,","type":"Skill","start_idx":365,"end_idx":371},{"text":"Map","type":"Skill","start_idx":372,"end_idx":375},{"text":"Map Reduce,","type":"Skill","start_idx":372,"end_idx":383},{"text":"Reduce,","type":"Skill","start_idx":376,"end_idx":383},{"text":"Flume,","type":"Skill","start_idx":384,"end_idx":390},{"text":"Oozie.","type":"Skill","start_idx":391,"end_idx":397}]},{"text":"Strong knowledge in HIVE and PIG core functionality by using custom User Defined Function's (UDF), User Defined Table-Generating Functions (UDTF) and User Defined Aggregating Functions (UDAF) for Hive.","entities":[{"text":"HIVE","type":"Skill","start_idx":20,"end_idx":24},{"text":"PIG","type":"Skill","start_idx":29,"end_idx":32},{"text":"User Defined Function's (UDF),","type":"Skill","start_idx":68,"end_idx":98},{"text":"User Defined Table-Generating Functions (UDTF)","type":"Skill","start_idx":99,"end_idx":145},{"text":"User Defined Aggregating Functions (UDAF)","type":"Skill","start_idx":150,"end_idx":191},{"text":"Hive.","type":"Skill","start_idx":196,"end_idx":201}]},{"text":"Experience on Productionizing Apache Nifi.","entities":[{"text":"Apache Nifi.","type":"Skill","start_idx":30,"end_idx":42}]},{"text":"for dataflows with significant processing requirements and controlling security of data flow.","entities":[{"text":"data flow.","type":"Skill","start_idx":83,"end_idx":93}]},{"text":"Designed and developed RDD Seeds using Scala and Cascading.","entities":[{"text":"Designed","type":"Skill","start_idx":0,"end_idx":8},{"text":"developed","type":"Skill","start_idx":13,"end_idx":22},{"text":"RDD Seeds","type":"Skill","start_idx":23,"end_idx":32},{"text":"Scala","type":"Skill","start_idx":39,"end_idx":44},{"text":"Cascading.","type":"Skill","start_idx":49,"end_idx":59}]},{"text":"Streaming data to Sparkstreaming using Kafka Exposure to Spark, Spark Streaming, Spark MLlib, Scala and Creating the Data Frames handled in Spark with Scala.","entities":[{"text":"Sparkstreaming","type":"Skill","start_idx":18,"end_idx":32},{"text":"Kafka Exposure","type":"Skill","start_idx":39,"end_idx":53},{"text":"Spark,","type":"Skill","start_idx":57,"end_idx":63},{"text":"Spark Streaming,","type":"Skill","start_idx":64,"end_idx":80},{"text":"Spark MLlib,","type":"Skill","start_idx":81,"end_idx":93},{"text":"Scala","type":"Skill","start_idx":94,"end_idx":99},{"text":"Data Frames","type":"Skill","start_idx":117,"end_idx":128},{"text":"Spark","type":"Skill","start_idx":140,"end_idx":145},{"text":"Scala.","type":"Skill","start_idx":151,"end_idx":157}]},{"text":"Good Exposure on Map Reduce programming using Java, PIG Latin Scripting and Distributed Application and HDFS.","entities":[{"text":"Map Reduce","type":"Skill","start_idx":17,"end_idx":27},{"text":"Java,","type":"Skill","start_idx":46,"end_idx":51},{"text":"PIG","type":"Skill","start_idx":52,"end_idx":55},{"text":"PIG Latin Scripting","type":"Skill","start_idx":52,"end_idx":71},{"text":"Latin","type":"Skill","start_idx":56,"end_idx":61},{"text":"Distributed Application","type":"Skill","start_idx":76,"end_idx":99}]},{"text":"Experienced Good understanding of NoSQL databases and hands on work experience in writing applications No SQL Databases HBase, Cassandra and MongoDB.","entities":[{"text":"NoSQL databases","type":"Skill","start_idx":34,"end_idx":49},{"text":"No SQL Databases HBase,","type":"Skill","start_idx":103,"end_idx":126},{"text":"Cassandra","type":"Skill","start_idx":127,"end_idx":136},{"text":"MongoDB.","type":"Skill","start_idx":141,"end_idx":149}]},{"text":"Very good implementation experience of Object Oriented concepts, Multithreading and Java/Scala Experienced with the Scala, Spark improving the performance and optimization of the existing algorithms in Hadoop using Spark Context, Spark -SQL, Pair RDD's, Spark YARN.","entities":[{"text":"Object Oriented concepts,","type":"Skill","start_idx":39,"end_idx":64},{"text":"Multithreading","type":"Skill","start_idx":65,"end_idx":79},{"text":"Java/Scala","type":"Skill","start_idx":84,"end_idx":94},{"text":"Scala,","type":"Skill","start_idx":116,"end_idx":122},{"text":"Spark","type":"Skill","start_idx":123,"end_idx":128},{"text":"Hadoop using Spark Context,","type":"Skill","start_idx":202,"end_idx":229},{"text":"Spark","type":"Skill","start_idx":230,"end_idx":235},{"text":"-SQL,","type":"Skill","start_idx":236,"end_idx":241},{"text":"Pair RDD's,","type":"Skill","start_idx":242,"end_idx":253},{"text":"Spark YARN.","type":"Skill","start_idx":254,"end_idx":265}]},{"text":"Experienced in installation, configuration, supporting and managing Hadoop Clusters using Apache Cloudera distributions, Horton works, Cloud Storage and Amazon web services (AWS) and related technologies DynamoDB, EMR, S3, ML.","entities":[{"text":"Hadoop Clusters","type":"Skill","start_idx":68,"end_idx":83},{"text":"Apache Cloudera","type":"Skill","start_idx":90,"end_idx":105},{"text":"Horton","type":"Skill","start_idx":121,"end_idx":127},{"text":"Cloud Storage","type":"Skill","start_idx":135,"end_idx":148},{"text":"Amazon web services (AWS)","type":"Skill","start_idx":153,"end_idx":178},{"text":"DynamoDB,","type":"Skill","start_idx":204,"end_idx":213},{"text":"EMR,","type":"Skill","start_idx":214,"end_idx":218},{"text":"S3,","type":"Skill","start_idx":219,"end_idx":222},{"text":"ML.","type":"Skill","start_idx":223,"end_idx":226}]},{"text":"Experience in deploying NiFi Data flow in Production team and Integrating data from multiple sources like Cassandra, MongoDB.","entities":[{"text":"NiFi Data flow","type":"Skill","start_idx":24,"end_idx":38},{"text":"Cassandra,","type":"Skill","start_idx":106,"end_idx":116},{"text":"MongoDB.","type":"Skill","start_idx":117,"end_idx":125}]},{"text":"Deploying templates to environments can be done via NiFi RestAPI integrated with other automation tools Complete end to end design and development of Apache NiFi flow which acts as the agent between middleware team and EBI team and executes all the actions mentioned above Experienced in Python programming, wrote Web Crawlers using Python.","entities":[{"text":"NiFi RestAPI","type":"Skill","start_idx":52,"end_idx":64},{"text":"development","type":"Skill","start_idx":135,"end_idx":146},{"text":"Apache NiFi","type":"Skill","start_idx":150,"end_idx":161},{"text":"EBI team","type":"Skill","start_idx":219,"end_idx":227},{"text":"Python programming,","type":"Skill","start_idx":288,"end_idx":307},{"text":"wrote Web Crawlers using Python.","type":"Skill","start_idx":308,"end_idx":340}]},{"text":"Experience in bench marking Hadoop cluster for analysis of queue usage Experienced in working with Mahout for applying machine learning techniques in the Hadoop Ecosystem.","entities":[{"text":"Hadoop cluster","type":"Skill","start_idx":28,"end_idx":42},{"text":"queue","type":"Skill","start_idx":59,"end_idx":64},{"text":"Mahout","type":"Skill","start_idx":99,"end_idx":105},{"text":"machine learning","type":"Skill","start_idx":119,"end_idx":135},{"text":"Hadoop Ecosystem.","type":"Skill","start_idx":154,"end_idx":171}]},{"text":"Good Experience on Amazon Web Services like Redshift, Data Pipeline, ML.","entities":[{"text":"Amazon Web Services","type":"Skill","start_idx":19,"end_idx":38},{"text":"Redshift,","type":"Skill","start_idx":44,"end_idx":53},{"text":"Data Pipeline,","type":"Skill","start_idx":54,"end_idx":68},{"text":"ML.","type":"Skill","start_idx":69,"end_idx":72}]},{"text":"Good experienced on moving the data in and out of Hadoop RDBMS, No-SQL and UNIX from various systems using SQOOP and other traditional data movement technologies.","entities":[{"text":"Hadoop RDBMS,","type":"Skill","start_idx":50,"end_idx":63},{"text":"No-SQL","type":"Skill","start_idx":64,"end_idx":70},{"text":"UNIX","type":"Skill","start_idx":75,"end_idx":79},{"text":"SQOOP","type":"Skill","start_idx":107,"end_idx":112}]},{"text":"Experience on Integration of Quartz scheduler with Oozie work flows to get data from multiple data sources in parallel using fork.","entities":[{"text":"Integration","type":"Skill","start_idx":14,"end_idx":25},{"text":"Quartz","type":"Skill","start_idx":29,"end_idx":35},{"text":"Oozie","type":"Skill","start_idx":51,"end_idx":56}]},{"text":"Experience in installation, configuration, support and management of a Hadoop Cluster using Cloudera Distributions.","entities":[{"text":"installation,","type":"Skill","start_idx":14,"end_idx":27},{"text":"configuration,","type":"Skill","start_idx":28,"end_idx":42},{"text":"support","type":"Skill","start_idx":43,"end_idx":50},{"text":"management","type":"Skill","start_idx":55,"end_idx":65},{"text":"Hadoop Cluster","type":"Skill","start_idx":71,"end_idx":85},{"text":"Cloudera Distributions.","type":"Skill","start_idx":92,"end_idx":115}]},{"text":"Experienced Spark scripts by using Scala shell as per requirements.","entities":[{"text":"Spark","type":"Skill","start_idx":12,"end_idx":17},{"text":"Scala","type":"Skill","start_idx":35,"end_idx":40}]},{"text":"Good knowledge on tuning the Spark jobs by changing the configuration properties and using broadcast variables.","entities":[{"text":"Spark","type":"Skill","start_idx":29,"end_idx":34}]},{"text":"Developed REST APIs using Java, Play framework and Akka.","entities":[{"text":"REST APIs","type":"Skill","start_idx":10,"end_idx":19},{"text":"Java,","type":"Skill","start_idx":26,"end_idx":31},{"text":"Play","type":"Skill","start_idx":32,"end_idx":36},{"text":"Play framework","type":"Skill","start_idx":32,"end_idx":46},{"text":"Akka.","type":"Skill","start_idx":51,"end_idx":56}]},{"text":"Expertise in search technology's like SOLR, Informatica & Lucene.","entities":[{"text":"SOLR,","type":"Skill","start_idx":38,"end_idx":43},{"text":"Informatica & Lucene.","type":"Skill","start_idx":44,"end_idx":65}]},{"text":"Experience in converting SQL queries into Spark Transformations using Spark RDDs and Scala and Performed map-side joins on RDD's.","entities":[{"text":"SQL queries","type":"Skill","start_idx":25,"end_idx":36},{"text":"Spark Transformations","type":"Skill","start_idx":42,"end_idx":63},{"text":"Spark RDDs","type":"Skill","start_idx":70,"end_idx":80},{"text":"Scala","type":"Skill","start_idx":85,"end_idx":90},{"text":"Performed map-side joins","type":"Skill","start_idx":95,"end_idx":119},{"text":"RDD's.","type":"Skill","start_idx":123,"end_idx":129}]},{"text":"Experienced in writing Hadoop Jobs for analyzing data using Hive Query Language (HQL), Pig Latin (Data flow language), and custom MapReduce programs in Java.","entities":[{"text":"Hadoop Jobs","type":"Skill","start_idx":23,"end_idx":34},{"text":"Hive Query Language (HQL),","type":"Skill","start_idx":60,"end_idx":86},{"text":"Pig Latin (Data flow language),","type":"Skill","start_idx":87,"end_idx":118},{"text":"MapReduce programs in Java.","type":"Skill","start_idx":130,"end_idx":157}]},{"text":"Good understanding of NoSQL databases like MongoDB, Cassandra, and HBase.","entities":[{"text":"NoSQL databases","type":"Skill","start_idx":22,"end_idx":37},{"text":"MongoDB, Cassandra, and HBase.","type":"Skill","start_idx":43,"end_idx":73}]},{"text":"Strong analytical skills with ability to quickly understand client's business needs.","entities":[{"text":"analytical skills","type":"Skill","start_idx":7,"end_idx":24},{"text":"business needs.","type":"Skill","start_idx":69,"end_idx":84}]},{"text":"Involved in business meetings for requirements gathering form business clients.","entities":[{"text":"business clients.","type":"Skill","start_idx":62,"end_idx":79}]},{"text":"Experienced in Storm builder topologies to perform cleansing operations before moving data into HBase.","entities":[{"text":"Storm","type":"Skill","start_idx":15,"end_idx":20},{"text":"moving data","type":"Skill","start_idx":79,"end_idx":90},{"text":"HBase.","type":"Skill","start_idx":96,"end_idx":102}]},{"text":"Hands on experience in configuring and working with Flume to load the data from multiple sources directly into Hdfs.","entities":[{"text":"Flume","type":"Skill","start_idx":52,"end_idx":57},{"text":"Hdfs.","type":"Skill","start_idx":111,"end_idx":116}]},{"text":"Experience on configuring fully the Flume agent, suitable for all type of logger data and store them in Avro Sink in Parquet file format and developing 2-tier architecture connecting channels between Avro sinks and Source.","entities":[{"text":"Flume agent,","type":"Skill","start_idx":36,"end_idx":48},{"text":"Avro Sink","type":"Skill","start_idx":104,"end_idx":113},{"text":"Parquet file format","type":"Skill","start_idx":117,"end_idx":136},{"text":"Avro sinks","type":"Skill","start_idx":200,"end_idx":210},{"text":"Source.","type":"Skill","start_idx":215,"end_idx":222}]},{"text":"Experience creating Visual report, Graphical analysis and Dashboard reports using Tableau, Informatica of historical data saved in Hdfs and data analysis using Splunk enterprise edition.","entities":[{"text":"Visual report,","type":"Skill","start_idx":20,"end_idx":34},{"text":"Graphical analysis","type":"Skill","start_idx":35,"end_idx":53},{"text":"Dashboard reports","type":"Skill","start_idx":58,"end_idx":75},{"text":"Tableau,","type":"Skill","start_idx":82,"end_idx":90},{"text":"Informatica of historical data","type":"Skill","start_idx":91,"end_idx":121},{"text":"Hdfs","type":"Skill","start_idx":131,"end_idx":135},{"text":"Splunk enterprise edition.","type":"Skill","start_idx":160,"end_idx":186}]},{"text":"Good experience in utilizing Cloud Storage Services like Git.","entities":[{"text":"Cloud Storage Services","type":"Skill","start_idx":29,"end_idx":51},{"text":"Git.","type":"Skill","start_idx":57,"end_idx":61}]},{"text":"Extensive knowledge in using GitHub and Bit Bucket.","entities":[{"text":"GitHub","type":"Skill","start_idx":29,"end_idx":35},{"text":"Bit Bucket.","type":"Skill","start_idx":40,"end_idx":51}]},{"text":"Experienced in job scheduling and monitoring using Oozie, Zookeeper.","entities":[{"text":"job scheduling","type":"Skill","start_idx":15,"end_idx":29},{"text":"Oozie,","type":"Skill","start_idx":51,"end_idx":57},{"text":"Zookeeper.","type":"Skill","start_idx":58,"end_idx":68}]},{"text":"Work Experience Sr. Spark Developer CenterPoint Energy - Houston, TX June 2018 to Present Responsibilities: Hands on experience in installation, configuration, supporting and managing Hadoop Clusters.","entities":[{"text":"Sr. Spark Developer","type":"Job Title","start_idx":16,"end_idx":35},{"text":"CenterPoint Energy","type":"Company Name","start_idx":36,"end_idx":54},{"text":"Houston,","type":"Location","start_idx":57,"end_idx":65},{"text":"TX","type":"Location","start_idx":66,"end_idx":68},{"text":"June 2018","type":"Date","start_idx":69,"end_idx":78},{"text":"Present","type":"Date","start_idx":82,"end_idx":89},{"text":"installation,","type":"Skill","start_idx":131,"end_idx":144},{"text":"configuration,","type":"Skill","start_idx":145,"end_idx":159},{"text":"supporting","type":"Skill","start_idx":160,"end_idx":170},{"text":"managing Hadoop Clusters.","type":"Skill","start_idx":175,"end_idx":200}]},{"text":"Knowledge of Cassandra security, maintenance and tuning both database and server.","entities":[{"text":"Cassandra security,","type":"Skill","start_idx":13,"end_idx":32},{"text":"database","type":"Skill","start_idx":61,"end_idx":69},{"text":"server.","type":"Skill","start_idx":74,"end_idx":81}]},{"text":"Chipped away at outlining and building up the Real Time Analysis module for Analytic Dashboard utilizing Cassandra, Kafka, Spark Streaming.","entities":[{"text":"Real Time Analysis","type":"Skill","start_idx":46,"end_idx":64},{"text":"Analytic Dashboard","type":"Skill","start_idx":76,"end_idx":94},{"text":"Cassandra,","type":"Skill","start_idx":105,"end_idx":115},{"text":"Kafka,","type":"Skill","start_idx":116,"end_idx":122},{"text":"Spark","type":"Skill","start_idx":123,"end_idx":128}]},{"text":"Installed and configured Confluent Kafka in R&D line.","entities":[{"text":"Confluent Kafka","type":"Skill","start_idx":25,"end_idx":40},{"text":"R&D","type":"Skill","start_idx":44,"end_idx":47}]},{"text":"Validated the installation with HDFS connector and Hive connectors.","entities":[{"text":"HDFS connector","type":"Skill","start_idx":32,"end_idx":46},{"text":"Hive connectors.","type":"Skill","start_idx":51,"end_idx":67}]},{"text":"Deployed high availability on the Hadoop cluster quorum journal nodes.","entities":[{"text":"Hadoop cluster","type":"Skill","start_idx":34,"end_idx":48}]},{"text":"Experience on implementing SAX (Symbolic Aggregate approXimation) in Java to use with Apache Spark for normalizing time series data.","entities":[{"text":"SAX","type":"Skill","start_idx":27,"end_idx":30},{"text":"SAX (Symbolic Aggregate approXimation)","type":"Skill","start_idx":27,"end_idx":65},{"text":"Java","type":"Skill","start_idx":69,"end_idx":73},{"text":"Apache Spark","type":"Skill","start_idx":86,"end_idx":98}]},{"text":"Involved in defining job flows, managing and reviewing log file.","entities":[]},{"text":"Set-up configured and optimized the Cassandra cluster.","entities":[{"text":"Cassandra cluster.","type":"Skill","start_idx":36,"end_idx":54}]},{"text":"Developed real-time Spark based application to work along with the Cassandra database.","entities":[{"text":"Spark","type":"Skill","start_idx":20,"end_idx":25},{"text":"Cassandra database.","type":"Skill","start_idx":67,"end_idx":86}]},{"text":"Responsible to manage data coming from different sources through Kafka.","entities":[{"text":"Kafka.","type":"Skill","start_idx":65,"end_idx":71}]},{"text":"Installed Kafka Producer on different severs and Scheduled to produce data for every 10 seconds Integrated Kafka with Spark Streaming to listen onto multiple Kafka Brokers with different Kafka topics for every 5 Seconds.","entities":[{"text":"Kafka Producer","type":"Skill","start_idx":10,"end_idx":24},{"text":"Kafka","type":"Skill","start_idx":107,"end_idx":112},{"text":"Spark Streaming","type":"Skill","start_idx":118,"end_idx":133},{"text":"Kafka Brokers","type":"Skill","start_idx":158,"end_idx":171},{"text":"Kafka","type":"Skill","start_idx":187,"end_idx":192}]},{"text":"Enhanced and optimized product Spark code to aggregate, group and run data mining tasks using the Spark framework and handled Json Data.","entities":[{"text":"Spark","type":"Skill","start_idx":31,"end_idx":36},{"text":"data mining","type":"Skill","start_idx":70,"end_idx":81},{"text":"Json Data.","type":"Skill","start_idx":126,"end_idx":136}]},{"text":"Handled Json Data comes from Kafka Direct Stream on each partitions and transformed them into required Data Frame Formats.","entities":[{"text":"Json Data","type":"Skill","start_idx":8,"end_idx":17},{"text":"Kafka Direct Stream","type":"Skill","start_idx":29,"end_idx":48},{"text":"Data Frame Formats.","type":"Skill","start_idx":103,"end_idx":122}]},{"text":"Upgraded Spark 1.6 to latest Version Spark 2.2 and configure Kafka Version 0.10.","entities":[{"text":"Spark 1.6","type":"Skill","start_idx":9,"end_idx":18},{"text":"Spark 2.2","type":"Skill","start_idx":37,"end_idx":46},{"text":"Kafka Version 0.10.","type":"Skill","start_idx":61,"end_idx":80}]},{"text":"Managing Kafka Offsets, Saving Offsets in external data base like HBase and to its own Kafka.","entities":[{"text":"Kafka Offsets,","type":"Skill","start_idx":9,"end_idx":23},{"text":"Saving Offsets","type":"Skill","start_idx":24,"end_idx":38},{"text":"HBase","type":"Skill","start_idx":66,"end_idx":71},{"text":"Kafka.","type":"Skill","start_idx":87,"end_idx":93}]},{"text":"Worked on Import & Export of data using ETL tool Sqoop from MySQL to HDFS.","entities":[{"text":"ETL tool Sqoop","type":"Skill","start_idx":40,"end_idx":54},{"text":"MySQL to HDFS.","type":"Skill","start_idx":60,"end_idx":74}]},{"text":"Worked on Lambda Architecture for both Batch processing and Real Streaming purposes.","entities":[{"text":"Lambda Architecture","type":"Skill","start_idx":10,"end_idx":29},{"text":"Batch processing","type":"Skill","start_idx":39,"end_idx":55},{"text":"Real Streaming purposes.","type":"Skill","start_idx":60,"end_idx":84}]},{"text":"Used Oozie to Schedule Spark and Kafka Producer Jobs to run in parallel.","entities":[{"text":"Oozie","type":"Skill","start_idx":5,"end_idx":10},{"text":"Schedule Spark","type":"Skill","start_idx":14,"end_idx":28},{"text":"Kafka Producer Jobs","type":"Skill","start_idx":33,"end_idx":52}]},{"text":"Appended the Data Frames into Cassandra Key Space Tables using DataStax Spark-Cassandra Connector.","entities":[{"text":"Data Frames","type":"Skill","start_idx":13,"end_idx":24},{"text":"Cassandra Key Space Tables","type":"Skill","start_idx":30,"end_idx":56},{"text":"DataStax Spark-Cassandra Connector.","type":"Skill","start_idx":63,"end_idx":98}]},{"text":"Experience with Cassandra YAML, Configuration files, RACK DC properties file, Cassandra-env file for JMX configurations etc. Installed and configured Datastax OpsCenter and Nagios for Cassandra cluster maintenance and alert.","entities":[{"text":"Cassandra YAML,","type":"Skill","start_idx":16,"end_idx":31},{"text":"Configuration files,","type":"Skill","start_idx":32,"end_idx":52},{"text":"RACK DC properties file,","type":"Skill","start_idx":53,"end_idx":77},{"text":"Cassandra-env file for JMX configurations","type":"Skill","start_idx":78,"end_idx":119},{"text":"Installed","type":"Skill","start_idx":125,"end_idx":134},{"text":"configured","type":"Skill","start_idx":139,"end_idx":149},{"text":"Datastax OpsCenter","type":"Skill","start_idx":150,"end_idx":168},{"text":"Nagios for Cassandra","type":"Skill","start_idx":173,"end_idx":193},{"text":"cluster maintenance and alert.","type":"Skill","start_idx":194,"end_idx":224}]},{"text":"Configured Authentication and security in Apache kafka pub-sub system.","entities":[{"text":"Authentication","type":"Skill","start_idx":11,"end_idx":25},{"text":"security","type":"Skill","start_idx":30,"end_idx":38},{"text":"Apache kafka pub-sub system.","type":"Skill","start_idx":42,"end_idx":70}]},{"text":"Good experience with Century Link Cloud for provisioning virtual machines, creating resource groups, configuring key vaults for storing encryption keys, Monitoring etc. Great Hands on Experience in seat stamping Hadoop bunch for investigation of line utilization Performing OS level setups and Kernel level tuning Implement and test integration of BI (Business Intelligence) tools with Hadoop stack.","entities":[{"text":"Century Link Cloud","type":"Skill","start_idx":21,"end_idx":39},{"text":"encryption keys,","type":"Skill","start_idx":136,"end_idx":152},{"text":"Monitoring","type":"Skill","start_idx":153,"end_idx":163},{"text":"Hadoop","type":"Skill","start_idx":212,"end_idx":218},{"text":"Performing OS level setups","type":"Skill","start_idx":263,"end_idx":289},{"text":"Kernel","type":"Skill","start_idx":294,"end_idx":300},{"text":"BI (Business Intelligence) tools","type":"Skill","start_idx":348,"end_idx":380},{"text":"Hadoop stack.","type":"Skill","start_idx":386,"end_idx":399}]},{"text":"Installed/Configured/Maintained Apache Hadoop clusters for application development and Hadoop tools like Hive, Pig, HBase, Zookeeper, Sqoop, Yarn, Spark2, Kafka and Oozie.","entities":[{"text":"Apache Hadoop","type":"Skill","start_idx":32,"end_idx":45},{"text":"Apache Hadoop clusters","type":"Skill","start_idx":32,"end_idx":54},{"text":"Hive,","type":"Skill","start_idx":105,"end_idx":110},{"text":"Pig,","type":"Skill","start_idx":111,"end_idx":115},{"text":"HBase,","type":"Skill","start_idx":116,"end_idx":122},{"text":"Zookeeper,","type":"Skill","start_idx":123,"end_idx":133},{"text":"Sqoop,","type":"Skill","start_idx":134,"end_idx":140},{"text":"Yarn,","type":"Skill","start_idx":141,"end_idx":146},{"text":"Spark2,","type":"Skill","start_idx":147,"end_idx":154},{"text":"Kafka","type":"Skill","start_idx":155,"end_idx":160},{"text":"Oozie.","type":"Skill","start_idx":165,"end_idx":171}]},{"text":"Formulated procedures for installation of Hadoop, Spark2 patches, updates and version upgrades.","entities":[{"text":"Hadoop,","type":"Skill","start_idx":42,"end_idx":49},{"text":"Spark2 patches,","type":"Skill","start_idx":50,"end_idx":65},{"text":"updates","type":"Skill","start_idx":66,"end_idx":73},{"text":"version upgrades.","type":"Skill","start_idx":78,"end_idx":95}]},{"text":"Environment: Cloudera, HDFS, Spark, Hive, Pig, Map Reduce, Hue, Sqoop, Putt, Apache Kafka, Apache Drill, Century Link Cloud, AWS, Java Netezza, Cassandra, Oozie, Spark , SPARK SQL, Maven, SBT, Java, Scala, SQL and Linux, YARN, Agile Methodology, Solr, PHP Admin, XAMPP, DataStax Cassandra.","entities":[{"text":"Cloudera,","type":"Skill","start_idx":13,"end_idx":22},{"text":"HDFS,","type":"Skill","start_idx":23,"end_idx":28},{"text":"Spark,","type":"Skill","start_idx":29,"end_idx":35},{"text":"Hive,","type":"Skill","start_idx":36,"end_idx":41},{"text":"Pig,","type":"Skill","start_idx":42,"end_idx":46},{"text":"Map","type":"Skill","start_idx":47,"end_idx":50},{"text":"Reduce,","type":"Skill","start_idx":51,"end_idx":58},{"text":"Hue,","type":"Skill","start_idx":59,"end_idx":63},{"text":"Sqoop,","type":"Skill","start_idx":64,"end_idx":70},{"text":"Putt,","type":"Skill","start_idx":71,"end_idx":76},{"text":"Apache","type":"Skill","start_idx":77,"end_idx":83},{"text":"Kafka,","type":"Skill","start_idx":84,"end_idx":90},{"text":"Apache Drill,","type":"Skill","start_idx":91,"end_idx":104},{"text":"Century Link Cloud,","type":"Skill","start_idx":105,"end_idx":124},{"text":"AWS,","type":"Skill","start_idx":125,"end_idx":129},{"text":"Java","type":"Skill","start_idx":130,"end_idx":134},{"text":"Netezza,","type":"Skill","start_idx":135,"end_idx":143},{"text":"Cassandra,","type":"Skill","start_idx":144,"end_idx":154},{"text":"Oozie,","type":"Skill","start_idx":155,"end_idx":161},{"text":"Spark","type":"Skill","start_idx":162,"end_idx":167},{"text":"SPARK SQL,","type":"Skill","start_idx":170,"end_idx":180},{"text":"Maven,","type":"Skill","start_idx":181,"end_idx":187},{"text":"SBT,","type":"Skill","start_idx":188,"end_idx":192},{"text":"Java,","type":"Skill","start_idx":193,"end_idx":198},{"text":"Scala,","type":"Skill","start_idx":199,"end_idx":205},{"text":"SQL","type":"Skill","start_idx":206,"end_idx":209},{"text":"Linux,","type":"Skill","start_idx":214,"end_idx":220},{"text":"YARN,","type":"Skill","start_idx":221,"end_idx":226},{"text":"Agile Methodology,","type":"Skill","start_idx":227,"end_idx":245},{"text":"Solr,","type":"Skill","start_idx":246,"end_idx":251},{"text":"PHP Admin,","type":"Skill","start_idx":252,"end_idx":262},{"text":"XAMPP,","type":"Skill","start_idx":263,"end_idx":269},{"text":"DataStax Cassandra.","type":"Skill","start_idx":270,"end_idx":289}]},{"text":"Sr. Hadoop/Spark Developer CPS Energy - San Antonio, TX January 2017 to June 2018 Responsibilities: Involved in deploying systems on Amazon Web Services (AWS) Infrastructure services EC2.","entities":[{"text":"Sr. Hadoop/Spark Developer","type":"Job Title","start_idx":0,"end_idx":26},{"text":"CPS Energy","type":"Company Name","start_idx":27,"end_idx":37},{"text":"San Antonio,","type":"Location","start_idx":40,"end_idx":52},{"text":"TX","type":"Location","start_idx":53,"end_idx":55},{"text":"January 2017","type":"Date","start_idx":56,"end_idx":68},{"text":"June 2018","type":"Date","start_idx":72,"end_idx":81},{"text":"Amazon Web Services (AWS)","type":"Skill","start_idx":133,"end_idx":158},{"text":"EC2.","type":"Skill","start_idx":183,"end_idx":187}]},{"text":"Experience in configuring, deploying the web applications on AWS servers using SBT and Play.","entities":[{"text":"AWS","type":"Skill","start_idx":61,"end_idx":64},{"text":"SBT","type":"Skill","start_idx":79,"end_idx":82},{"text":"Play.","type":"Skill","start_idx":87,"end_idx":92}]},{"text":"Migrated Map Reduce jobs into Spark RDD transformations using Scala.","entities":[{"text":"Map Reduce","type":"Skill","start_idx":9,"end_idx":19},{"text":"Spark RDD","type":"Skill","start_idx":30,"end_idx":39},{"text":"Scala.","type":"Skill","start_idx":62,"end_idx":68}]},{"text":"Used Spark API over Cloudera Hadoop YARN to perform analytics on data in Hive.","entities":[{"text":"Spark API over","type":"Skill","start_idx":5,"end_idx":19},{"text":"Cloudera Hadoop","type":"Skill","start_idx":20,"end_idx":35},{"text":"YARN","type":"Skill","start_idx":36,"end_idx":40},{"text":"analytics","type":"Skill","start_idx":52,"end_idx":61},{"text":"Hive.","type":"Skill","start_idx":73,"end_idx":78}]},{"text":"Developed Spark code using Spark RDD and Spark-SQL/Streaming for faster processing of data.","entities":[{"text":"Spark code","type":"Skill","start_idx":10,"end_idx":20},{"text":"Spark RDD","type":"Skill","start_idx":27,"end_idx":36},{"text":"Spark-SQL/Streaming","type":"Skill","start_idx":41,"end_idx":60}]},{"text":"Performed configuration, deployment and support of cloud services including Amazon Web Services (AWS).","entities":[{"text":"configuration,","type":"Skill","start_idx":10,"end_idx":24},{"text":"deployment","type":"Skill","start_idx":25,"end_idx":35},{"text":"support of cloud services","type":"Skill","start_idx":40,"end_idx":65},{"text":"Amazon Web Services (AWS).","type":"Skill","start_idx":76,"end_idx":102}]},{"text":"Working knowledge of various AWS technologies like SQS Queuing, SNS Notification, S3 storage, Redshift, Data Pipeline, EMR.","entities":[{"text":"AWS","type":"Skill","start_idx":29,"end_idx":32},{"text":"SQS Queuing,","type":"Skill","start_idx":51,"end_idx":63},{"text":"SNS Notification,","type":"Skill","start_idx":64,"end_idx":81},{"text":"S3 storage,","type":"Skill","start_idx":82,"end_idx":93},{"text":"Redshift,","type":"Skill","start_idx":94,"end_idx":103},{"text":"Data Pipeline,","type":"Skill","start_idx":104,"end_idx":118},{"text":"EMR.","type":"Skill","start_idx":119,"end_idx":123}]},{"text":"Responsible for all Public (AWS) and Private (Openstack/VMWare/DCOS/Mesos/Marathon) cloud infrastructure Developed Flume ETL job for handling data from HTTP Source and Sink as HDFS and configuring Data Pipelining.","entities":[{"text":"(AWS)","type":"Skill","start_idx":27,"end_idx":32},{"text":"(Openstack/VMWare/DCOS/Mesos/Marathon)","type":"Skill","start_idx":45,"end_idx":83},{"text":"cloud infrastructure","type":"Skill","start_idx":84,"end_idx":104},{"text":"Flume ETL","type":"Skill","start_idx":115,"end_idx":124},{"text":"HTTP Source","type":"Skill","start_idx":152,"end_idx":163},{"text":"Sink","type":"Skill","start_idx":168,"end_idx":172},{"text":"HDFS","type":"Skill","start_idx":176,"end_idx":180},{"text":"Data Pipelining.","type":"Skill","start_idx":197,"end_idx":213}]},{"text":"Used Hive data warehouse tool to analyze the unified historic data in HDFS to identify issues and behavioral patterns.","entities":[{"text":"Hive","type":"Skill","start_idx":5,"end_idx":9},{"text":"data warehouse","type":"Skill","start_idx":10,"end_idx":24},{"text":"HDFS","type":"Skill","start_idx":70,"end_idx":74}]},{"text":"Involved in Developing a Restful service using Python Flask framework.","entities":[{"text":"Restful","type":"Skill","start_idx":25,"end_idx":32},{"text":"Python Flask framework.","type":"Skill","start_idx":47,"end_idx":70}]},{"text":"Expertise in working with Python GUI frameworks - PyJamas, Jython.","entities":[{"text":"Python GUI","type":"Skill","start_idx":26,"end_idx":36},{"text":"PyJamas,","type":"Skill","start_idx":50,"end_idx":58},{"text":"Jython.","type":"Skill","start_idx":59,"end_idx":66}]},{"text":"Experienced in using Apache Drill data-intensive distributed applications for interactive analysis of large-scale datasets.","entities":[{"text":"Apache Drill","type":"Skill","start_idx":21,"end_idx":33},{"text":"large-scale datasets.","type":"Skill","start_idx":102,"end_idx":123}]},{"text":"Developed end to end ETL batch and streaming data integration into Hadoop(MapR), transforming data.","entities":[{"text":"ETL batch","type":"Skill","start_idx":21,"end_idx":30},{"text":"Hadoop(MapR),","type":"Skill","start_idx":67,"end_idx":80}]},{"text":"Used Python modules such as requests, urllib, urllib2 for web crawling.","entities":[{"text":"Python modules","type":"Skill","start_idx":5,"end_idx":19},{"text":"requests,","type":"Skill","start_idx":28,"end_idx":37},{"text":"urllib,","type":"Skill","start_idx":38,"end_idx":45},{"text":"urllib2","type":"Skill","start_idx":46,"end_idx":53},{"text":"web crawling.","type":"Skill","start_idx":58,"end_idx":71}]},{"text":"Tools developed extensively include Spark, Drill, Hive, HBase, Kafka & MapR Streams, PostgreSQL, Stream Sets.","entities":[{"text":"Spark,","type":"Skill","start_idx":36,"end_idx":42},{"text":"Drill,","type":"Skill","start_idx":43,"end_idx":49},{"text":"Hive,","type":"Skill","start_idx":50,"end_idx":55},{"text":"HBase,","type":"Skill","start_idx":56,"end_idx":62},{"text":"Kafka","type":"Skill","start_idx":63,"end_idx":68},{"text":"&","type":"Skill","start_idx":69,"end_idx":70},{"text":"MapR Streams,","type":"Skill","start_idx":71,"end_idx":84},{"text":"PostgreSQL,","type":"Skill","start_idx":85,"end_idx":96},{"text":"Stream Sets.","type":"Skill","start_idx":97,"end_idx":109}]},{"text":"Used Hive Queries in Spark-SQL for analysis and processing the data.","entities":[{"text":"Hive Queries","type":"Skill","start_idx":5,"end_idx":17},{"text":"Spark-SQL","type":"Skill","start_idx":21,"end_idx":30},{"text":"analysis","type":"Skill","start_idx":35,"end_idx":43}]},{"text":"Worked as a key role in a team of developing an initial prototype of a NiFi big data pipeline.","entities":[{"text":"NiFi big data pipeline.","type":"Skill","start_idx":71,"end_idx":94}]},{"text":"This pipeline demonstrated an end to end scenario of data ingestion, processing.","entities":[]},{"text":"Used HUE for running Hive queries.","entities":[{"text":"HUE","type":"Skill","start_idx":5,"end_idx":8},{"text":"Hive queries.","type":"Skill","start_idx":21,"end_idx":34}]},{"text":"Created Partitions according to day using Hive to improve performance.","entities":[{"text":"Partitions","type":"Skill","start_idx":8,"end_idx":18},{"text":"Hive","type":"Skill","start_idx":42,"end_idx":46}]},{"text":"Wrote Python routines to log into the websites and fetch data for selected options.","entities":[{"text":"Python","type":"Skill","start_idx":6,"end_idx":12}]},{"text":"Worked on custom Pig Loaders and storage classes to work with variety of data formats such as JSON and XML file formats.","entities":[{"text":"Pig Loaders","type":"Skill","start_idx":17,"end_idx":28},{"text":"JSON","type":"Skill","start_idx":94,"end_idx":98},{"text":"XML","type":"Skill","start_idx":103,"end_idx":106}]},{"text":"Loaded some of the data into Cassandra for fast retrieval of data.","entities":[{"text":"Cassandra","type":"Skill","start_idx":29,"end_idx":38}]},{"text":"Worked in provisioning and managing multi-tenant Hadoop clusters on public cloud environment - Amazon Web Services (AWS) and on private cloud infrastructure - Open stack cloud platform and worked on DynamoDB, Ml.","entities":[{"text":"multi-tenant Hadoop","type":"Skill","start_idx":36,"end_idx":55},{"text":"public cloud environment","type":"Skill","start_idx":68,"end_idx":92},{"text":"Amazon Web Services (AWS)","type":"Skill","start_idx":95,"end_idx":120},{"text":"Open stack cloud platform","type":"Skill","start_idx":159,"end_idx":184},{"text":"DynamoDB, Ml.","type":"Skill","start_idx":199,"end_idx":212}]},{"text":"Worked on large-scale Hadoop YARN cluster for distributed data processing and analysis using Data Bricks Connectors, Spark core, Spark SQL, Sqoop, Pig, Hive, Impala and NoSQL databases.","entities":[{"text":"Hadoop YARN","type":"Skill","start_idx":22,"end_idx":33},{"text":"Data Bricks","type":"Skill","start_idx":93,"end_idx":104},{"text":"Data Bricks Connectors,","type":"Skill","start_idx":93,"end_idx":116},{"text":"Spark core,","type":"Skill","start_idx":117,"end_idx":128},{"text":"Spark SQL,","type":"Skill","start_idx":129,"end_idx":139},{"text":"Sqoop, Pig,","type":"Skill","start_idx":140,"end_idx":151},{"text":"Hive,","type":"Skill","start_idx":152,"end_idx":157},{"text":"Impala","type":"Skill","start_idx":158,"end_idx":164},{"text":"NoSQL databases.","type":"Skill","start_idx":169,"end_idx":185}]},{"text":"Created HBase tables to load large sets of structured, semi-structured and unstructured data coming from UNIX, NoSQL and a variety of portfolios.","entities":[{"text":"HBase","type":"Skill","start_idx":8,"end_idx":13},{"text":"UNIX,","type":"Skill","start_idx":105,"end_idx":110},{"text":"NoSQL","type":"Skill","start_idx":111,"end_idx":116}]},{"text":"Worked on a POC to compare processing time of Impala with Apache Hive for batch applications to implement the former in project.","entities":[{"text":"POC","type":"Skill","start_idx":12,"end_idx":15},{"text":"Impala","type":"Skill","start_idx":46,"end_idx":52},{"text":"Apache Hive","type":"Skill","start_idx":58,"end_idx":69}]},{"text":"Worked with various HDFS file formats like Avro, Sequence File and various compression formats like Snappy, bzip2.","entities":[{"text":"HDFS","type":"Skill","start_idx":20,"end_idx":24},{"text":"Avro,","type":"Skill","start_idx":43,"end_idx":48},{"text":"Sequence File","type":"Skill","start_idx":49,"end_idx":62},{"text":"Snappy,","type":"Skill","start_idx":100,"end_idx":107},{"text":"bzip2.","type":"Skill","start_idx":108,"end_idx":114}]},{"text":"Used the RegEx, JSON and Avro for serialization and de-serialization packaged with Hive to parse the contents of streamed log data.","entities":[{"text":"RegEx,","type":"Skill","start_idx":9,"end_idx":15},{"text":"JSON","type":"Skill","start_idx":16,"end_idx":20},{"text":"Avro","type":"Skill","start_idx":25,"end_idx":29}]},{"text":"Converted all the vap processing from Netezza and implemented by using Spark data frames and RDD's.","entities":[{"text":"Netezza","type":"Skill","start_idx":38,"end_idx":45},{"text":"Spark data frames","type":"Skill","start_idx":71,"end_idx":88},{"text":"RDD's.","type":"Skill","start_idx":93,"end_idx":99}]},{"text":"Worked in writing Spark Sql scripts for optimizing the query performance.","entities":[{"text":"Spark Sql","type":"Skill","start_idx":18,"end_idx":27}]},{"text":"Responsible for handling different data formats like Avro, Parquet and ORC formats.","entities":[{"text":"Avro,","type":"Skill","start_idx":53,"end_idx":58},{"text":"Parquet","type":"Skill","start_idx":59,"end_idx":66},{"text":"ORC","type":"Skill","start_idx":71,"end_idx":74}]},{"text":"Implemented Spark Scripts using Scala, Spark SQL to access hive tables into Spark for faster processing of data.","entities":[{"text":"Spark Scripts","type":"Skill","start_idx":12,"end_idx":25},{"text":"Scala,","type":"Skill","start_idx":32,"end_idx":38},{"text":"Spark SQL","type":"Skill","start_idx":39,"end_idx":48},{"text":"Spark","type":"Skill","start_idx":76,"end_idx":81}]},{"text":"Environment: Cloudera, Horton Works distribution, HDFS, Spark, Hive, Pig, Map Reduce, Hue, Sqoop, Putty, HaaS (Hadoop as a Service), Apache Kafka, Apache Mesos and the AWS, Java Netezza, Cassandra, Oozie, Spark, SPARK SQL, Maven, Java, Scala, SQL and Linux, Toad, YARN, Agile Methodology.","entities":[{"text":"Cloudera,","type":"Skill","start_idx":13,"end_idx":22},{"text":"Horton Works distribution,","type":"Skill","start_idx":23,"end_idx":49},{"text":"HDFS,","type":"Skill","start_idx":50,"end_idx":55},{"text":"Spark,","type":"Skill","start_idx":56,"end_idx":62},{"text":"Hive,","type":"Skill","start_idx":63,"end_idx":68},{"text":"Pig,","type":"Skill","start_idx":69,"end_idx":73},{"text":"Map","type":"Skill","start_idx":74,"end_idx":77},{"text":"Reduce,","type":"Skill","start_idx":78,"end_idx":85},{"text":"Hue,","type":"Skill","start_idx":86,"end_idx":90},{"text":"Sqoop,","type":"Skill","start_idx":91,"end_idx":97},{"text":"Putty,","type":"Skill","start_idx":98,"end_idx":104},{"text":"HaaS","type":"Skill","start_idx":105,"end_idx":109},{"text":"(Hadoop as a Service),","type":"Skill","start_idx":110,"end_idx":132},{"text":"Apache","type":"Skill","start_idx":133,"end_idx":139},{"text":"Kafka,","type":"Skill","start_idx":140,"end_idx":146},{"text":"Apache","type":"Skill","start_idx":147,"end_idx":153},{"text":"Mesos","type":"Skill","start_idx":154,"end_idx":159},{"text":"AWS,","type":"Skill","start_idx":168,"end_idx":172},{"text":"Java","type":"Skill","start_idx":173,"end_idx":177},{"text":"Java Netezza,","type":"Skill","start_idx":173,"end_idx":186},{"text":"Cassandra,","type":"Skill","start_idx":187,"end_idx":197},{"text":"Oozie,","type":"Skill","start_idx":198,"end_idx":204},{"text":"Spark,","type":"Skill","start_idx":205,"end_idx":211},{"text":"SPARK SQL,","type":"Skill","start_idx":212,"end_idx":222},{"text":"Maven,","type":"Skill","start_idx":223,"end_idx":229},{"text":"Java,","type":"Skill","start_idx":230,"end_idx":235},{"text":"Scala,","type":"Skill","start_idx":236,"end_idx":242},{"text":"SQL","type":"Skill","start_idx":243,"end_idx":246},{"text":"Linux,","type":"Skill","start_idx":251,"end_idx":257},{"text":"Toad,","type":"Skill","start_idx":258,"end_idx":263},{"text":"YARN,","type":"Skill","start_idx":264,"end_idx":269},{"text":"Agile Methodology.","type":"Skill","start_idx":270,"end_idx":288}]},{"text":"Hadoop Developer BANK of America - Dallas, TX May 2016 to December 2016 Responsibilities: Concerned and well-informed on Hadoop Components such as HDFS, Job Tracker, Task Tracker, Name Node, Data Node, YARN and Map Reduce programming.","entities":[{"text":"Hadoop Developer","type":"Job Title","start_idx":0,"end_idx":16},{"text":"BANK of America","type":"Company Name","start_idx":17,"end_idx":32},{"text":"Dallas,","type":"Location","start_idx":35,"end_idx":42},{"text":"TX","type":"Location","start_idx":43,"end_idx":45},{"text":"May 2016","type":"Date","start_idx":46,"end_idx":54},{"text":"December 2016","type":"Date","start_idx":58,"end_idx":71},{"text":"Hadoop","type":"Skill","start_idx":121,"end_idx":127},{"text":"HDFS,","type":"Skill","start_idx":147,"end_idx":152},{"text":"Job Tracker,","type":"Skill","start_idx":153,"end_idx":165},{"text":"Task Tracker,","type":"Skill","start_idx":166,"end_idx":179},{"text":"Name Node,","type":"Skill","start_idx":180,"end_idx":190},{"text":"Data Node,","type":"Skill","start_idx":191,"end_idx":201},{"text":"YARN","type":"Skill","start_idx":202,"end_idx":206},{"text":"Map Reduce","type":"Skill","start_idx":211,"end_idx":221}]},{"text":"Developed Map-Reduce programs to get rid of irregularities and aggregate the data.","entities":[{"text":"Map-Reduce programs","type":"Skill","start_idx":10,"end_idx":29},{"text":"aggregate the data.","type":"Skill","start_idx":63,"end_idx":82}]},{"text":"Developed Cluster coordination services through Zookeeper.","entities":[{"text":"Cluster","type":"Skill","start_idx":10,"end_idx":17},{"text":"Zookeeper.","type":"Skill","start_idx":48,"end_idx":58}]},{"text":"Implemented Hive UDF's and did performance tuning for better results Developed Pig Latin Scripts to extract data from log files and store them to HDFS.","entities":[{"text":"Hive UDF's","type":"Skill","start_idx":12,"end_idx":22},{"text":"performance","type":"Skill","start_idx":31,"end_idx":42},{"text":"Pig Latin Scripts","type":"Skill","start_idx":79,"end_idx":96},{"text":"HDFS.","type":"Skill","start_idx":146,"end_idx":151}]},{"text":"Created User Defined Functions (UDFs) to pre-process data for analysis Implemented Optimized Map Joins to get data from different sources to perform cleaning operations before applying the algorithms.","entities":[{"text":"User Defined Functions (UDFs)","type":"Skill","start_idx":8,"end_idx":37},{"text":"Optimized Map Joins","type":"Skill","start_idx":83,"end_idx":102}]},{"text":"Created highly optimized SQL queries for MapReduce jobs, seamlessly matching the query to the appropriate Hive table configuration to generate efficient report.","entities":[{"text":"SQL queries","type":"Skill","start_idx":25,"end_idx":36},{"text":"MapReduce jobs,","type":"Skill","start_idx":41,"end_idx":56}]},{"text":"Used other packages such as Beautifulsoup for data parsing in Python.","entities":[{"text":"Beautifulsoup","type":"Skill","start_idx":28,"end_idx":41},{"text":"data parsing","type":"Skill","start_idx":46,"end_idx":58},{"text":"Python.","type":"Skill","start_idx":62,"end_idx":69}]},{"text":"Tuned, and developed SQL on HiveQL, Drill and SparkSQL Experience in using Sqoop to import and export the data from Oracle DB into HDFS and HIVE, HBase.","entities":[{"text":"Tuned,","type":"Skill","start_idx":0,"end_idx":6},{"text":"developed","type":"Skill","start_idx":11,"end_idx":20},{"text":"SQL","type":"Skill","start_idx":21,"end_idx":24},{"text":"HiveQL,","type":"Skill","start_idx":28,"end_idx":35},{"text":"Drill","type":"Skill","start_idx":36,"end_idx":41},{"text":"SparkSQL","type":"Skill","start_idx":46,"end_idx":54},{"text":"Sqoop","type":"Skill","start_idx":75,"end_idx":80},{"text":"Oracle DB","type":"Skill","start_idx":116,"end_idx":125},{"text":"HDFS","type":"Skill","start_idx":131,"end_idx":135},{"text":"HIVE,","type":"Skill","start_idx":140,"end_idx":145},{"text":"HBase.","type":"Skill","start_idx":146,"end_idx":152}]},{"text":"Implemented CRUD operations on HBase data using thrift API to get real time insights.","entities":[{"text":"CRUD","type":"Skill","start_idx":12,"end_idx":16},{"text":"HBase data","type":"Skill","start_idx":31,"end_idx":41},{"text":"API","type":"Skill","start_idx":55,"end_idx":58}]},{"text":"Developed workflow in Oozie to manage and schedule jobs on Hadoop cluster for generating reports on nightly, weekly and monthly basis.","entities":[{"text":"Oozie","type":"Skill","start_idx":22,"end_idx":27},{"text":"Hadoop cluster","type":"Skill","start_idx":59,"end_idx":73}]},{"text":"Worked on integration independent microservices for real-time bidding (scala/akka, firebase, cassandra, Elasticsearch) Used slick to query and storing in database in a Scala fashion using the powerful Scala collection framework Using HIVE processed extensively ETL loadings on a Structured Data.","entities":[{"text":"microservices","type":"Skill","start_idx":34,"end_idx":47},{"text":"(scala/akka,","type":"Skill","start_idx":70,"end_idx":82},{"text":"firebase,","type":"Skill","start_idx":83,"end_idx":92},{"text":"cassandra,","type":"Skill","start_idx":93,"end_idx":103},{"text":"Elasticsearch)","type":"Skill","start_idx":104,"end_idx":118},{"text":"Scala","type":"Skill","start_idx":168,"end_idx":173},{"text":"Scala","type":"Skill","start_idx":201,"end_idx":206},{"text":"HIVE","type":"Skill","start_idx":234,"end_idx":238},{"text":"ETL","type":"Skill","start_idx":261,"end_idx":264},{"text":"Structured Data.","type":"Skill","start_idx":279,"end_idx":295}]},{"text":"Defined job flows and developed simple to complex Map Reduce jobs as per the requirement.","entities":[{"text":"Map Reduce","type":"Skill","start_idx":50,"end_idx":60}]},{"text":"Optimized Map/Reduce Jobs to use HDFS efficiently by using various compression mechanisms.","entities":[{"text":"Optimized Map/Reduce","type":"Skill","start_idx":0,"end_idx":20},{"text":"HDFS","type":"Skill","start_idx":33,"end_idx":37}]},{"text":"Developed PIG UDFs for manipulating the data according to Business Requirements and also worked on developing custom PIG Loaders.","entities":[{"text":"PIG","type":"Skill","start_idx":10,"end_idx":13},{"text":"UDFs","type":"Skill","start_idx":14,"end_idx":18},{"text":"Business Requirements","type":"Skill","start_idx":58,"end_idx":79},{"text":"PIG Loaders.","type":"Skill","start_idx":117,"end_idx":129}]},{"text":"Created various Parser programs to extract data from Autosys, Tibco Business Objects, XML, Informatica, Java, and database views using Scala PIG UDF was required to extract the information of the area from the huge data which we get from the sensors.","entities":[{"text":"Autosys,","type":"Skill","start_idx":53,"end_idx":61},{"text":"Tibco Business","type":"Skill","start_idx":62,"end_idx":76},{"text":"Objects,","type":"Skill","start_idx":77,"end_idx":85},{"text":"XML,","type":"Skill","start_idx":86,"end_idx":90},{"text":"Informatica,","type":"Skill","start_idx":91,"end_idx":103},{"text":"Java,","type":"Skill","start_idx":104,"end_idx":109},{"text":"Scala","type":"Skill","start_idx":135,"end_idx":140},{"text":"PIG UDF","type":"Skill","start_idx":141,"end_idx":148}]},{"text":"Responsible for creating Hive tables based on business requirements.","entities":[{"text":"Hive tables","type":"Skill","start_idx":25,"end_idx":36},{"text":"business requirements.","type":"Skill","start_idx":46,"end_idx":68}]},{"text":"Implemented Partitioning, Dynamic Partitions and Buckets in HIVE for efficient data access.","entities":[{"text":"Partitioning,","type":"Skill","start_idx":12,"end_idx":25},{"text":"Dynamic Partitions","type":"Skill","start_idx":26,"end_idx":44},{"text":"Buckets","type":"Skill","start_idx":49,"end_idx":56},{"text":"HIVE","type":"Skill","start_idx":60,"end_idx":64}]},{"text":"Involved in NoSQL database design, integration and implementation.","entities":[{"text":"NoSQL","type":"Skill","start_idx":12,"end_idx":17}]},{"text":"Loaded data into NoSQL database HBase.","entities":[{"text":"NoSQL database HBase.","type":"Skill","start_idx":17,"end_idx":38}]},{"text":"Worked on debugging, performance tuning PIG and HIVE scripts by understanding the joins, group and aggregation between them.","entities":[{"text":"PIG","type":"Skill","start_idx":40,"end_idx":43},{"text":"HIVE","type":"Skill","start_idx":48,"end_idx":52}]},{"text":"Used Flume to collect, aggregate and store the web log data from different sources like web servers and pushed to HDFS.","entities":[{"text":"Flume","type":"Skill","start_idx":5,"end_idx":10},{"text":"HDFS.","type":"Skill","start_idx":114,"end_idx":119}]},{"text":"Connected the hive tables to Data analyzing tools like Tableau for Graphical representation of the trends.","entities":[{"text":"Tableau for Graphical","type":"Skill","start_idx":55,"end_idx":76}]},{"text":"Experienced in managing and reviewing Hadoop log files.","entities":[{"text":"Hadoop log files.","type":"Skill","start_idx":38,"end_idx":55}]},{"text":"Involved in loading data from UNIX file system to HDFS.","entities":[{"text":"UNIX","type":"Skill","start_idx":30,"end_idx":34},{"text":"HDFS.","type":"Skill","start_idx":50,"end_idx":55}]},{"text":"Responsible for design & development of Spark SQL Scripts based on Functional Specifications.","entities":[{"text":"Spark SQL","type":"Skill","start_idx":40,"end_idx":49}]},{"text":"Used Apache HUE interface to monitor and manage the HDFS storage","entities":[{"text":"Apache HUE","type":"Skill","start_idx":5,"end_idx":15},{"text":"HDFS storage","type":"Skill","start_idx":52,"end_idx":64}]},{"text":"Concerned and well-informed on Hadoop Components such as HDFS, Job Tracker, Task Tracker, Name Node, Data Node, YARN and Map Reduce programming.","entities":[{"text":"HDFS,","type":"Skill","start_idx":57,"end_idx":62},{"text":"Job Tracker,","type":"Skill","start_idx":63,"end_idx":75},{"text":"Task Tracker,","type":"Skill","start_idx":76,"end_idx":89},{"text":"Name Node,","type":"Skill","start_idx":90,"end_idx":100},{"text":"Data Node,","type":"Skill","start_idx":101,"end_idx":111},{"text":"YARN","type":"Skill","start_idx":112,"end_idx":116},{"text":"Map Reduce programming.","type":"Skill","start_idx":121,"end_idx":144}]},{"text":"Developed Map-Reduce programs to get rid of irregularities and aggregate the data.","entities":[]},{"text":"Developed Cluster coordination services through Zookeeper.","entities":[{"text":"Zookeeper.","type":"Skill","start_idx":48,"end_idx":58}]},{"text":"Implemented Hive UDF's and did performance tuning for better results Developed Pig Latin Scripts to extract data from log files and store them to HDFS.","entities":[{"text":"Hive UDF's","type":"Skill","start_idx":12,"end_idx":22},{"text":"Pig Latin Scripts","type":"Skill","start_idx":79,"end_idx":96},{"text":"HDFS.","type":"Skill","start_idx":146,"end_idx":151}]},{"text":"Created User Defined Functions (UDFs) to pre-process data for analysis Implemented Optimized Map Joins to get data from different sources to perform cleaning operations before applying the algorithms.","entities":[{"text":"Created User Defined Functions (UDFs)","type":"Skill","start_idx":0,"end_idx":37},{"text":"Optimized Map Joins","type":"Skill","start_idx":83,"end_idx":102}]},{"text":"Created highly optimized SQL queries for MapReduce jobs, seamlessly matching the query to the appropriate Hive table configuration to generate efficient report.","entities":[{"text":"SQL","type":"Skill","start_idx":25,"end_idx":28},{"text":"MapReduce jobs,","type":"Skill","start_idx":41,"end_idx":56}]},{"text":"Used other packages such as Beautifulsoup for data parsing in Python.","entities":[{"text":"Beautifulsoup","type":"Skill","start_idx":28,"end_idx":41},{"text":"data parsing","type":"Skill","start_idx":46,"end_idx":58},{"text":"Python.","type":"Skill","start_idx":62,"end_idx":69}]},{"text":"Tuned, and developed SQL on HiveQL, Drill and SparkSQL Experience in using Sqoop to import and export the data from Oracle DB into HDFS and HIVE, HBase.","entities":[{"text":"SQL","type":"Skill","start_idx":21,"end_idx":24},{"text":"HiveQL,","type":"Skill","start_idx":28,"end_idx":35},{"text":"Drill","type":"Skill","start_idx":36,"end_idx":41},{"text":"SparkSQL","type":"Skill","start_idx":46,"end_idx":54},{"text":"Sqoop","type":"Skill","start_idx":75,"end_idx":80},{"text":"Oracle DB","type":"Skill","start_idx":116,"end_idx":125},{"text":"HDFS","type":"Skill","start_idx":131,"end_idx":135},{"text":"HIVE,","type":"Skill","start_idx":140,"end_idx":145},{"text":"HBase.","type":"Skill","start_idx":146,"end_idx":152}]},{"text":"Implemented CRUD operations on HBase data using thrift API to get real time insights.","entities":[{"text":"CRUD","type":"Skill","start_idx":12,"end_idx":16},{"text":"HBase","type":"Skill","start_idx":31,"end_idx":36},{"text":"API","type":"Skill","start_idx":55,"end_idx":58}]},{"text":"Developed workflow in Oozie to manage and schedule jobs on Hadoop cluster for generating reports on nightly, weekly and monthly basis.","entities":[{"text":"Oozie","type":"Skill","start_idx":22,"end_idx":27},{"text":"Hadoop cluster","type":"Skill","start_idx":59,"end_idx":73}]},{"text":"Worked on integration independent microservices for real-time bidding (scala/akka, firebase, cassandra, Elasticsearch) Used slick to query and storing in database in a Scala fashion using the powerful Scala collection framework Using HIVE processed extensively ETL loadings on a Structured Data.","entities":[{"text":"microservices","type":"Skill","start_idx":34,"end_idx":47},{"text":"(scala/akka,","type":"Skill","start_idx":70,"end_idx":82},{"text":"firebase,","type":"Skill","start_idx":83,"end_idx":92},{"text":"cassandra,","type":"Skill","start_idx":93,"end_idx":103},{"text":"Elasticsearch)","type":"Skill","start_idx":104,"end_idx":118},{"text":"Scala","type":"Skill","start_idx":168,"end_idx":173},{"text":"Scala","type":"Skill","start_idx":201,"end_idx":206},{"text":"HIVE","type":"Skill","start_idx":234,"end_idx":238},{"text":"ETL","type":"Skill","start_idx":261,"end_idx":264}]},{"text":"Defined job flows and developed simple to complex Map Reduce jobs as per the requirement.","entities":[{"text":"Map Reduce jobs","type":"Skill","start_idx":50,"end_idx":65}]},{"text":"Optimized Map/Reduce Jobs to use HDFS efficiently by using various compression mechanisms.","entities":[{"text":"Map/Reduce","type":"Skill","start_idx":10,"end_idx":20},{"text":"HDFS","type":"Skill","start_idx":33,"end_idx":37},{"text":"compression mechanisms.","type":"Skill","start_idx":67,"end_idx":90}]},{"text":"Developed PIG UDFs for manipulating the data according to Business Requirements and also worked on developing custom PIG Loaders.","entities":[{"text":"PIG UDFs","type":"Skill","start_idx":10,"end_idx":18},{"text":"Business Requirements","type":"Skill","start_idx":58,"end_idx":79},{"text":"PIG Loaders.","type":"Skill","start_idx":117,"end_idx":129}]},{"text":"Created various Parser programs to extract data from Autosys, Tibco Business Objects, XML, Informatica, Java, and database views using Scala PIG UDF was required to extract the information of the area from the huge data which we get from the sensors.","entities":[{"text":"Autosys,","type":"Skill","start_idx":53,"end_idx":61},{"text":"Tibco Business Objects,","type":"Skill","start_idx":62,"end_idx":85},{"text":"XML,","type":"Skill","start_idx":86,"end_idx":90},{"text":"Informatica,","type":"Skill","start_idx":91,"end_idx":103},{"text":"Java,","type":"Skill","start_idx":104,"end_idx":109},{"text":"Scala","type":"Skill","start_idx":135,"end_idx":140},{"text":"PIG UDF","type":"Skill","start_idx":141,"end_idx":148}]},{"text":"Responsible for creating Hive tables based on business requirements.","entities":[{"text":"business requirements.","type":"Skill","start_idx":46,"end_idx":68}]},{"text":"Implemented Partitioning, Dynamic Partitions and Buckets in HIVE for efficient data access.","entities":[{"text":"Implemented Partitioning,","type":"Skill","start_idx":0,"end_idx":25},{"text":"Dynamic Partitions","type":"Skill","start_idx":26,"end_idx":44},{"text":"Buckets in HIVE","type":"Skill","start_idx":49,"end_idx":64}]},{"text":"Involved in NoSQL database design, integration and implementation.","entities":[{"text":"NoSQL","type":"Skill","start_idx":12,"end_idx":17},{"text":"database","type":"Skill","start_idx":18,"end_idx":26},{"text":"database design,","type":"Skill","start_idx":18,"end_idx":34},{"text":"integration","type":"Skill","start_idx":35,"end_idx":46},{"text":"implementation.","type":"Skill","start_idx":51,"end_idx":66}]},{"text":"Loaded data into NoSQL database HBase.","entities":[{"text":"NoSQL database HBase.","type":"Skill","start_idx":17,"end_idx":38}]},{"text":"Worked on debugging, performance tuning PIG and HIVE scripts by understanding the joins, group and aggregation between them.","entities":[{"text":"PIG","type":"Skill","start_idx":40,"end_idx":43},{"text":"HIVE","type":"Skill","start_idx":48,"end_idx":52}]},{"text":"Used Flume to collect, aggregate and store the web log data from different sources like web servers and pushed to HDFS.","entities":[{"text":"Flume","type":"Skill","start_idx":5,"end_idx":10},{"text":"collect,","type":"Skill","start_idx":14,"end_idx":22},{"text":"aggregate","type":"Skill","start_idx":23,"end_idx":32},{"text":"store","type":"Skill","start_idx":37,"end_idx":42},{"text":"web log data","type":"Skill","start_idx":47,"end_idx":59},{"text":"HDFS.","type":"Skill","start_idx":114,"end_idx":119}]},{"text":"Connected the hive tables to Data analyzing tools like Tableau for Graphical representation of the trends.","entities":[{"text":"Data analyzing","type":"Skill","start_idx":29,"end_idx":43},{"text":"Tableau","type":"Skill","start_idx":55,"end_idx":62}]},{"text":"Experienced in managing and reviewing Hadoop log files.","entities":[]},{"text":"Involved in loading data from UNIX file system to HDFS.","entities":[{"text":"UNIX","type":"Skill","start_idx":30,"end_idx":34},{"text":"HDFS.","type":"Skill","start_idx":50,"end_idx":55}]},{"text":"Responsible for design & development of Spark SQL Scripts based on Functional Specifications.","entities":[{"text":"Spark SQL","type":"Skill","start_idx":40,"end_idx":49}]},{"text":"Used Apache HUE interface to monitor and manage the HDFS storage.","entities":[{"text":"Apache HUE","type":"Skill","start_idx":5,"end_idx":15},{"text":"HDFS","type":"Skill","start_idx":52,"end_idx":56}]},{"text":"Environment: HDFS, Map Reduce, Pig, Mesos, AWS Hive, Sqoop, Scala, Flume, Mahout, HBase, Spark, SPARK SQL, Yarn, Java, Maven, Git, Cloudera, MongoDB, Eclipse and Shell Scripting.","entities":[{"text":"HDFS,","type":"Skill","start_idx":13,"end_idx":18},{"text":"Map Reduce,","type":"Skill","start_idx":19,"end_idx":30},{"text":"Pig,","type":"Skill","start_idx":31,"end_idx":35},{"text":"Mesos,","type":"Skill","start_idx":36,"end_idx":42},{"text":"AWS","type":"Skill","start_idx":43,"end_idx":46},{"text":"Hive,","type":"Skill","start_idx":47,"end_idx":52},{"text":"Sqoop,","type":"Skill","start_idx":53,"end_idx":59},{"text":"Scala,","type":"Skill","start_idx":60,"end_idx":66},{"text":"Flume,","type":"Skill","start_idx":67,"end_idx":73},{"text":"Mahout,","type":"Skill","start_idx":74,"end_idx":81},{"text":"HBase,","type":"Skill","start_idx":82,"end_idx":88},{"text":"Spark,","type":"Skill","start_idx":89,"end_idx":95},{"text":"SPARK","type":"Skill","start_idx":96,"end_idx":101},{"text":"SQL,","type":"Skill","start_idx":102,"end_idx":106},{"text":"Yarn,","type":"Skill","start_idx":107,"end_idx":112},{"text":"Java,","type":"Skill","start_idx":113,"end_idx":118},{"text":"Maven,","type":"Skill","start_idx":119,"end_idx":125},{"text":"Git,","type":"Skill","start_idx":126,"end_idx":130},{"text":"Cloudera,","type":"Skill","start_idx":131,"end_idx":140},{"text":"MongoDB,","type":"Skill","start_idx":141,"end_idx":149},{"text":"Eclipse","type":"Skill","start_idx":150,"end_idx":157},{"text":"Shell Scripting.","type":"Skill","start_idx":162,"end_idx":178}]},{"text":"Hadoop Developer DELL - Bengaluru, Karnataka June 2013 to August 2015 Responsibilities: Designed and developed data movement framework for multiple sources like SQL Server, Oracle, and MySQL.","entities":[{"text":"Hadoop Developer","type":"Job Title","start_idx":0,"end_idx":16},{"text":"DELL","type":"Company Name","start_idx":17,"end_idx":21},{"text":"Bengaluru, Karnataka","type":"Location","start_idx":24,"end_idx":44},{"text":"June 2013","type":"Date","start_idx":45,"end_idx":54},{"text":"August 2015","type":"Date","start_idx":58,"end_idx":69},{"text":"SQL Server,","type":"Skill","start_idx":161,"end_idx":172},{"text":"Oracle,","type":"Skill","start_idx":173,"end_idx":180},{"text":"MySQL.","type":"Skill","start_idx":185,"end_idx":191}]},{"text":"Created Sqoop import and export jobs for multiple sources.","entities":[{"text":"Sqoop","type":"Skill","start_idx":8,"end_idx":13}]},{"text":"Developed scripts to automate the creation Sqoop jobs for various workflows.","entities":[{"text":"Sqoop","type":"Skill","start_idx":43,"end_idx":48}]},{"text":"Developed Hive scripts to alter the tables and perform required transformations.","entities":[{"text":"Hive scripts","type":"Skill","start_idx":10,"end_idx":22}]},{"text":"Developed a java MapReduce and PIG cleansers for data cleansing.","entities":[{"text":"java MapReduce","type":"Skill","start_idx":12,"end_idx":26},{"text":"PIG cleansers","type":"Skill","start_idx":31,"end_idx":44}]},{"text":"Worked on Hive UDFS to mask confidential information in the data.","entities":[{"text":"Hive UDFS","type":"Skill","start_idx":10,"end_idx":19},{"text":"confidential information","type":"Skill","start_idx":28,"end_idx":52}]},{"text":"Designed and developed MapReduce programs for data lineage.","entities":[{"text":"MapReduce","type":"Skill","start_idx":23,"end_idx":32}]},{"text":"Designed and developed the framework to log information for auditing and failure recovery.","entities":[]},{"text":"Closed worked with the web application development team to develop the user interface for data movement framework.","entities":[]},{"text":"Designed Oozie workflows for Job Automation.","entities":[]},{"text":"Created Map Reduce programs to handle semi/unstructured data like xml, Json, Avro data files and sequence files for log files.","entities":[{"text":"Map Reduce programs","type":"Skill","start_idx":8,"end_idx":27},{"text":"semi/unstructured data","type":"Skill","start_idx":38,"end_idx":60},{"text":"xml,","type":"Skill","start_idx":66,"end_idx":70},{"text":"Json,","type":"Skill","start_idx":71,"end_idx":76},{"text":"Avro data files","type":"Skill","start_idx":77,"end_idx":92},{"text":"sequence files for log files.","type":"Skill","start_idx":97,"end_idx":126}]},{"text":"A RESTful web service, built with python and cherrypy, retrieves data from an accumulo data warehouse Maintaining the MySQL server and Authentication to required users for Databases.","entities":[{"text":"RESTful","type":"Skill","start_idx":2,"end_idx":9},{"text":"web service,","type":"Skill","start_idx":10,"end_idx":22},{"text":"python","type":"Skill","start_idx":34,"end_idx":40},{"text":"cherrypy,","type":"Skill","start_idx":45,"end_idx":54},{"text":"data warehouse","type":"Skill","start_idx":87,"end_idx":101},{"text":"MySQL server","type":"Skill","start_idx":118,"end_idx":130},{"text":"Authentication","type":"Skill","start_idx":135,"end_idx":149},{"text":"Databases.","type":"Skill","start_idx":172,"end_idx":182}]},{"text":"Appropriately documented various Administrative technical issues.","entities":[]},{"text":"Developed MapReduce programs to extract and transform the data sets and results were exported back to RDBMS using Sqoop.","entities":[{"text":"MapReduce","type":"Skill","start_idx":10,"end_idx":19},{"text":"RDBMS","type":"Skill","start_idx":102,"end_idx":107},{"text":"Sqoop.","type":"Skill","start_idx":114,"end_idx":120}]},{"text":"Built a RESTful web service for storing and retrieving documents in an apache accumulo data store Involved in collecting and aggregating large amounts of log data using Apache Flume and staging data in HDFS for further analysis.","entities":[{"text":"RESTful web service","type":"Skill","start_idx":8,"end_idx":27},{"text":"Apache Flume","type":"Skill","start_idx":169,"end_idx":181},{"text":"HDFS","type":"Skill","start_idx":202,"end_idx":206}]},{"text":"Optimized our Hadoop infrastructure at both the Software and Hardware level.","entities":[{"text":"Hadoop","type":"Skill","start_idx":14,"end_idx":20},{"text":"Software and Hardware level.","type":"Skill","start_idx":48,"end_idx":76}]},{"text":"Experience in troubleshooting in MapReduce jobs by reviewing log files.","entities":[{"text":"MapReduce","type":"Skill","start_idx":33,"end_idx":42}]},{"text":"Developed end-to-end search solution using web crawler, Apache Nutch & Search Platform, Apache SOLR.","entities":[{"text":"Apache Nutch","type":"Skill","start_idx":56,"end_idx":68},{"text":"Apache SOLR.","type":"Skill","start_idx":88,"end_idx":100}]},{"text":"Environment:Hadoop, Cloudera Manager, Linux, RedHat, CentOs, Ubuntu Operating System, Scala, Map Reduce, HBase, SQL, Sqoop, HDFS, Kafka, UML, Apache SOLR, Hive, Oozie, Cassandra, maven, Pig, UNIX, Python, MR Unit, Git.","entities":[{"text":"Environment:Hadoop,","type":"Skill","start_idx":0,"end_idx":19},{"text":"Cloudera Manager,","type":"Skill","start_idx":20,"end_idx":37},{"text":"Linux,","type":"Skill","start_idx":38,"end_idx":44},{"text":"RedHat,","type":"Skill","start_idx":45,"end_idx":52},{"text":"CentOs,","type":"Skill","start_idx":53,"end_idx":60},{"text":"Ubuntu","type":"Skill","start_idx":61,"end_idx":67},{"text":"Operating","type":"Skill","start_idx":68,"end_idx":77},{"text":"System,","type":"Skill","start_idx":78,"end_idx":85},{"text":"Scala,","type":"Skill","start_idx":86,"end_idx":92},{"text":"Map","type":"Skill","start_idx":93,"end_idx":96},{"text":"Reduce,","type":"Skill","start_idx":97,"end_idx":104},{"text":"HBase,","type":"Skill","start_idx":105,"end_idx":111},{"text":"SQL,","type":"Skill","start_idx":112,"end_idx":116},{"text":"Sqoop,","type":"Skill","start_idx":117,"end_idx":123},{"text":"HDFS,","type":"Skill","start_idx":124,"end_idx":129},{"text":"Kafka,","type":"Skill","start_idx":130,"end_idx":136},{"text":"UML,","type":"Skill","start_idx":137,"end_idx":141},{"text":"Apache SOLR,","type":"Skill","start_idx":142,"end_idx":154},{"text":"Hive,","type":"Skill","start_idx":155,"end_idx":160},{"text":"Oozie,","type":"Skill","start_idx":161,"end_idx":167},{"text":"Cassandra,","type":"Skill","start_idx":168,"end_idx":178},{"text":"maven,","type":"Skill","start_idx":179,"end_idx":185},{"text":"Pig,","type":"Skill","start_idx":186,"end_idx":190},{"text":"UNIX,","type":"Skill","start_idx":191,"end_idx":196},{"text":"Python,","type":"Skill","start_idx":197,"end_idx":204},{"text":"MR","type":"Skill","start_idx":205,"end_idx":207},{"text":"Unit,","type":"Skill","start_idx":208,"end_idx":213},{"text":"Git.","type":"Skill","start_idx":214,"end_idx":218}]},{"text":"Java Developer STATE FARM - Bengaluru, Karnataka July 2011 to June 2013 Responsibilities: Developed the J2EE application based on the Service Oriented Architecture by employing SOAP and other tools for data exchanges and updates.","entities":[{"text":"Java Developer","type":"Job Title","start_idx":0,"end_idx":14},{"text":"STATE FARM","type":"Company Name","start_idx":15,"end_idx":25},{"text":"Bengaluru, Karnataka","type":"Location","start_idx":28,"end_idx":48},{"text":"July 2011","type":"Date","start_idx":49,"end_idx":58},{"text":"June 2013","type":"Date","start_idx":62,"end_idx":71},{"text":"J2EE","type":"Skill","start_idx":104,"end_idx":108},{"text":"Service Oriented Architecture","type":"Skill","start_idx":134,"end_idx":163},{"text":"SOAP","type":"Skill","start_idx":177,"end_idx":181}]},{"text":"Developed the functionalities using Agile Methodology.","entities":[{"text":"Agile Methodology.","type":"Skill","start_idx":36,"end_idx":54}]},{"text":"Used Apache Maven for project management and building the application.","entities":[{"text":"Apache Maven","type":"Skill","start_idx":5,"end_idx":17}]},{"text":"Worked in all the modules of the application which involved front-end presentation logic developed using Spring MVC, JSP, JSTL and JavaScript, Business objects developed using POJOs and data access layer using Hibernate framework.","entities":[{"text":"Spring MVC,","type":"Skill","start_idx":105,"end_idx":116},{"text":"JSP,","type":"Skill","start_idx":117,"end_idx":121},{"text":"JSTL","type":"Skill","start_idx":122,"end_idx":126},{"text":"JavaScript,","type":"Skill","start_idx":131,"end_idx":142},{"text":"POJOs","type":"Skill","start_idx":176,"end_idx":181},{"text":"Hibernate","type":"Skill","start_idx":210,"end_idx":219}]},{"text":"Used JAX-RS (REST) for producing web services and involved in writing programs to consume the web services with Apache CXF framework.","entities":[{"text":"JAX-RS (REST)","type":"Skill","start_idx":5,"end_idx":18},{"text":"web services","type":"Skill","start_idx":33,"end_idx":45},{"text":"writing programs","type":"Skill","start_idx":62,"end_idx":78},{"text":"Apache CXF framework.","type":"Skill","start_idx":112,"end_idx":133}]},{"text":"Used Restful API and SOAP web services for internal and external consumption.","entities":[{"text":"Restful API","type":"Skill","start_idx":5,"end_idx":16},{"text":"SOAP web services","type":"Skill","start_idx":21,"end_idx":38}]},{"text":"Used Spring ORM module for integration with Hibernate for persistence layer.","entities":[{"text":"Spring ORM","type":"Skill","start_idx":5,"end_idx":15},{"text":"integration","type":"Skill","start_idx":27,"end_idx":38},{"text":"Hibernate","type":"Skill","start_idx":44,"end_idx":53},{"text":"persistence layer.","type":"Skill","start_idx":58,"end_idx":76}]},{"text":"Involved in writing Hibernate Query Language (HQL) for persistence layer.","entities":[{"text":"Hibernate Query Language (HQL)","type":"Skill","start_idx":20,"end_idx":50},{"text":"persistence layer.","type":"Skill","start_idx":55,"end_idx":73}]},{"text":"Used Spring MVC, Spring AOP, Spring IOC, Spring Transaction and Oracle to create Club Systems Component.","entities":[{"text":"Spring MVC,","type":"Skill","start_idx":5,"end_idx":16},{"text":"Spring AOP,","type":"Skill","start_idx":17,"end_idx":28},{"text":"Spring IOC,","type":"Skill","start_idx":29,"end_idx":40},{"text":"Spring Transaction","type":"Skill","start_idx":41,"end_idx":59},{"text":"Oracle","type":"Skill","start_idx":64,"end_idx":70},{"text":"Club Systems Component.","type":"Skill","start_idx":81,"end_idx":104}]},{"text":"Wrote backend jobs based on Core Java & Oracle Data Base to be run daily/weekly.","entities":[{"text":"backend jobs","type":"Skill","start_idx":6,"end_idx":18},{"text":"Core Java","type":"Skill","start_idx":28,"end_idx":37},{"text":"Oracle Data Base","type":"Skill","start_idx":40,"end_idx":56}]},{"text":"Coding the core modules of the application compliant with the Java/J2EE coding standards and Design Patterns.","entities":[{"text":"Java/J2EE","type":"Skill","start_idx":62,"end_idx":71},{"text":"Design Patterns.","type":"Skill","start_idx":93,"end_idx":109}]},{"text":"Written Java Script, HTML, CSS, Servlets, and JSP for designing GUI of the application.","entities":[{"text":"Java","type":"Skill","start_idx":8,"end_idx":12},{"text":"Java Script,","type":"Skill","start_idx":8,"end_idx":20},{"text":"HTML,","type":"Skill","start_idx":21,"end_idx":26},{"text":"CSS,","type":"Skill","start_idx":27,"end_idx":31},{"text":"Servlets,","type":"Skill","start_idx":32,"end_idx":41},{"text":"JSP","type":"Skill","start_idx":46,"end_idx":49},{"text":"GUI","type":"Skill","start_idx":64,"end_idx":67}]},{"text":"Worked on Service-side and Middle-tier technologies, extracting catching strategies/solutions.","entities":[{"text":"Service-side","type":"Skill","start_idx":10,"end_idx":22}]},{"text":"Design data access layer using Data Access Layer J2EE patterns.","entities":[{"text":"J2EE","type":"Skill","start_idx":49,"end_idx":53}]},{"text":"Implementing the MVC architecture Struts Framework for handling databases across multiple locations and display information in presentation layer.","entities":[{"text":"MVC architecture","type":"Skill","start_idx":17,"end_idx":33},{"text":"Struts Framework","type":"Skill","start_idx":34,"end_idx":50}]},{"text":"Used XPath for parsing the XML elements as part of business logic processing.","entities":[{"text":"XPath","type":"Skill","start_idx":5,"end_idx":10},{"text":"XML","type":"Skill","start_idx":27,"end_idx":30},{"text":"business logic processing.","type":"Skill","start_idx":51,"end_idx":77}]},{"text":"Environment: Java, Struts 1.2, Hibernate 3.0, JSP, JavaScript, HTML, XML, Oracle, Eclipse, JBoss Application Server, ANT, CVS, and SQL.","entities":[{"text":"Java,","type":"Skill","start_idx":13,"end_idx":18},{"text":"Struts 1.2,","type":"Skill","start_idx":19,"end_idx":30},{"text":"Hibernate 3.0,","type":"Skill","start_idx":31,"end_idx":45},{"text":"JSP,","type":"Skill","start_idx":46,"end_idx":50},{"text":"JavaScript,","type":"Skill","start_idx":51,"end_idx":62},{"text":"HTML,","type":"Skill","start_idx":63,"end_idx":68},{"text":"XML,","type":"Skill","start_idx":69,"end_idx":73},{"text":"Oracle,","type":"Skill","start_idx":74,"end_idx":81},{"text":"Eclipse,","type":"Skill","start_idx":82,"end_idx":90},{"text":"JBoss","type":"Skill","start_idx":91,"end_idx":96},{"text":"Application","type":"Skill","start_idx":97,"end_idx":108},{"text":"Server,","type":"Skill","start_idx":109,"end_idx":116},{"text":"ANT,","type":"Skill","start_idx":117,"end_idx":121},{"text":"CVS,","type":"Skill","start_idx":122,"end_idx":126},{"text":"SQL.","type":"Skill","start_idx":131,"end_idx":135}]},{"text":"Education Masters Michigan State University - Ann Arbor, MI Skills Hdfs, Impala, Oozie, Sqoop, Apache kafka, Kafka, Db2, Flume, Jboss, Jms, Map reduce, Mongodb, Ms visual studio, Visual studio, Apache spark, Api, Hive, Html, Javascript, Node.js Additional Information TECHNICAL SKILLS: Big Data Ecosystems HDFS and Map Reduce, Pig, Hive, Pig Latin, Impala, YARN, Oozie, Zookeeper, Apache Spark, Apache Crunch, Apache NiFi, Apace STORM, Apache Kappa, Apache Kafka, Sqoop, Flume.","entities":[{"text":"Masters","type":"Degree","start_idx":10,"end_idx":17},{"text":"Michigan State University","type":"College","start_idx":18,"end_idx":43},{"text":"Ann Arbor,","type":"Location","start_idx":46,"end_idx":56},{"text":"MI","type":"Location","start_idx":57,"end_idx":59},{"text":"Hdfs,","type":"Skill","start_idx":67,"end_idx":72},{"text":"Impala,","type":"Skill","start_idx":73,"end_idx":80},{"text":"Oozie,","type":"Skill","start_idx":81,"end_idx":87},{"text":"Sqoop,","type":"Skill","start_idx":88,"end_idx":94},{"text":"Apache","type":"Skill","start_idx":95,"end_idx":101},{"text":"kafka,","type":"Skill","start_idx":102,"end_idx":108},{"text":"Kafka,","type":"Skill","start_idx":109,"end_idx":115},{"text":"Db2,","type":"Skill","start_idx":116,"end_idx":120},{"text":"Flume,","type":"Skill","start_idx":121,"end_idx":127},{"text":"Jboss,","type":"Skill","start_idx":128,"end_idx":134},{"text":"Jms,","type":"Skill","start_idx":135,"end_idx":139},{"text":"Map reduce,","type":"Skill","start_idx":140,"end_idx":151},{"text":"Mongodb,","type":"Skill","start_idx":152,"end_idx":160},{"text":"Ms visual studio,","type":"Skill","start_idx":161,"end_idx":178},{"text":"Visual studio,","type":"Skill","start_idx":179,"end_idx":193},{"text":"Apache spark,","type":"Skill","start_idx":194,"end_idx":207},{"text":"Api,","type":"Skill","start_idx":208,"end_idx":212},{"text":"Hive,","type":"Skill","start_idx":213,"end_idx":218},{"text":"Html,","type":"Skill","start_idx":219,"end_idx":224},{"text":"Javascript,","type":"Skill","start_idx":225,"end_idx":236},{"text":"Node.js","type":"Skill","start_idx":237,"end_idx":244},{"text":"Big Data Ecosystems","type":"Skill","start_idx":286,"end_idx":305},{"text":"HDFS and Map Reduce,","type":"Skill","start_idx":306,"end_idx":326},{"text":"Pig,","type":"Skill","start_idx":327,"end_idx":331},{"text":"Hive,","type":"Skill","start_idx":332,"end_idx":337},{"text":"Pig","type":"Skill","start_idx":338,"end_idx":341},{"text":"Latin,","type":"Skill","start_idx":342,"end_idx":348},{"text":"Impala,","type":"Skill","start_idx":349,"end_idx":356},{"text":"YARN,","type":"Skill","start_idx":357,"end_idx":362},{"text":"Oozie,","type":"Skill","start_idx":363,"end_idx":369},{"text":"Zookeeper,","type":"Skill","start_idx":370,"end_idx":380},{"text":"Apache","type":"Skill","start_idx":381,"end_idx":387},{"text":"Spark,","type":"Skill","start_idx":388,"end_idx":394},{"text":"Apache","type":"Skill","start_idx":395,"end_idx":401},{"text":"Crunch,","type":"Skill","start_idx":402,"end_idx":409},{"text":"Apache NiFi,","type":"Skill","start_idx":410,"end_idx":422},{"text":"Apace STORM,","type":"Skill","start_idx":423,"end_idx":435},{"text":"Apache Kappa,","type":"Skill","start_idx":436,"end_idx":449},{"text":"Apache Kafka,","type":"Skill","start_idx":450,"end_idx":463},{"text":"Sqoop,","type":"Skill","start_idx":464,"end_idx":470},{"text":"Flume.","type":"Skill","start_idx":471,"end_idx":477}]},{"text":"Streaming Technologies Spark Streaming, Storm Scripting Languages Python, Perl, Shell, Sheme, Tcl, Unix Shell Scripts, Windows Power Shell Programming Languages Java, J2EE, JDK1.4/1.5/1.6/1.7/1.8, JDBC, Hibernate, XML Parsers, JSP 1.2/2, Servlets, EJB, JMS, Struts, Spring Framework, Java Beans, AJAX, JNDI.","entities":[{"text":"Spark Streaming,","type":"Skill","start_idx":23,"end_idx":39},{"text":"Storm","type":"Skill","start_idx":40,"end_idx":45},{"text":"Python,","type":"Skill","start_idx":66,"end_idx":73},{"text":"Perl,","type":"Skill","start_idx":74,"end_idx":79},{"text":"Shell,","type":"Skill","start_idx":80,"end_idx":86},{"text":"Sheme,","type":"Skill","start_idx":87,"end_idx":93},{"text":"Tcl,","type":"Skill","start_idx":94,"end_idx":98},{"text":"Unix","type":"Skill","start_idx":99,"end_idx":103},{"text":"Shell Scripts,","type":"Skill","start_idx":104,"end_idx":118},{"text":"Windows Power Shell","type":"Skill","start_idx":119,"end_idx":138},{"text":"Java,","type":"Skill","start_idx":161,"end_idx":166},{"text":"J2EE,","type":"Skill","start_idx":167,"end_idx":172},{"text":"JDK1.4/1.5/1.6/1.7/1.8,","type":"Skill","start_idx":173,"end_idx":196},{"text":"JDBC,","type":"Skill","start_idx":197,"end_idx":202},{"text":"Hibernate,","type":"Skill","start_idx":203,"end_idx":213},{"text":"XML","type":"Skill","start_idx":214,"end_idx":217},{"text":"Parsers,","type":"Skill","start_idx":218,"end_idx":226},{"text":"JSP 1.2/2,","type":"Skill","start_idx":227,"end_idx":237},{"text":"Servlets,","type":"Skill","start_idx":238,"end_idx":247},{"text":"EJB,","type":"Skill","start_idx":248,"end_idx":252},{"text":"JMS,","type":"Skill","start_idx":253,"end_idx":257},{"text":"Struts,","type":"Skill","start_idx":258,"end_idx":265},{"text":"Spring","type":"Skill","start_idx":266,"end_idx":272},{"text":"Framework,","type":"Skill","start_idx":273,"end_idx":283},{"text":"Java","type":"Skill","start_idx":284,"end_idx":288},{"text":"Beans,","type":"Skill","start_idx":289,"end_idx":295},{"text":"AJAX,","type":"Skill","start_idx":296,"end_idx":301},{"text":"JNDI.","type":"Skill","start_idx":302,"end_idx":307}]},{"text":"Databases MongoDB, Netezza, SQL Server, MySQL, ORACLE, DB2 IDEs / Tools Eclipse, JUnit, Maven, Ant, MS Visual Studio, Net Beans Methodologies Agile, Waterfall Virtualization Technologies VMware ESXi, Windows Hyper-V, Power VM, Virtual box, Citrix Xen, KVM.","entities":[{"text":"MongoDB,","type":"Skill","start_idx":10,"end_idx":18},{"text":"Netezza,","type":"Skill","start_idx":19,"end_idx":27},{"text":"SQL","type":"Skill","start_idx":28,"end_idx":31},{"text":"Server,","type":"Skill","start_idx":32,"end_idx":39},{"text":"MySQL,","type":"Skill","start_idx":40,"end_idx":46},{"text":"ORACLE,","type":"Skill","start_idx":47,"end_idx":54},{"text":"DB2","type":"Skill","start_idx":55,"end_idx":58},{"text":"IDEs","type":"Skill","start_idx":59,"end_idx":63},{"text":"/","type":"Skill","start_idx":64,"end_idx":65},{"text":"Tools","type":"Skill","start_idx":66,"end_idx":71},{"text":"Eclipse,","type":"Skill","start_idx":72,"end_idx":80},{"text":"JUnit,","type":"Skill","start_idx":81,"end_idx":87},{"text":"Maven,","type":"Skill","start_idx":88,"end_idx":94},{"text":"Ant,","type":"Skill","start_idx":95,"end_idx":99},{"text":"MS","type":"Skill","start_idx":100,"end_idx":102},{"text":"Visual","type":"Skill","start_idx":103,"end_idx":109},{"text":"Studio,","type":"Skill","start_idx":110,"end_idx":117},{"text":"Net Beans","type":"Skill","start_idx":118,"end_idx":127},{"text":"Agile,","type":"Skill","start_idx":142,"end_idx":148},{"text":"Waterfall","type":"Skill","start_idx":149,"end_idx":158},{"text":"VMware","type":"Skill","start_idx":187,"end_idx":193},{"text":"ESXi,","type":"Skill","start_idx":194,"end_idx":199},{"text":"Windows","type":"Skill","start_idx":200,"end_idx":207},{"text":"Hyper-V,","type":"Skill","start_idx":208,"end_idx":216},{"text":"Power","type":"Skill","start_idx":217,"end_idx":222},{"text":"VM,","type":"Skill","start_idx":223,"end_idx":226},{"text":"Virtual","type":"Skill","start_idx":227,"end_idx":234},{"text":"box,","type":"Skill","start_idx":235,"end_idx":239},{"text":"Citrix","type":"Skill","start_idx":240,"end_idx":246},{"text":"Xen,","type":"Skill","start_idx":247,"end_idx":251},{"text":"KVM.","type":"Skill","start_idx":252,"end_idx":256}]},{"text":"Web Technologies HTML, JavaScript, JQuery, Ajax, Boot Strap, Angular JS, Node.js, Express.js Web Servers Web Logic, Web Sphere, Apache Tomcat, JBOSS.","entities":[{"text":"HTML,","type":"Skill","start_idx":17,"end_idx":22},{"text":"JavaScript,","type":"Skill","start_idx":23,"end_idx":34},{"text":"JQuery,","type":"Skill","start_idx":35,"end_idx":42},{"text":"Ajax,","type":"Skill","start_idx":43,"end_idx":48},{"text":"Boot","type":"Skill","start_idx":49,"end_idx":53},{"text":"Strap,","type":"Skill","start_idx":54,"end_idx":60},{"text":"Angular","type":"Skill","start_idx":61,"end_idx":68},{"text":"JS,","type":"Skill","start_idx":69,"end_idx":72},{"text":"Node.js,","type":"Skill","start_idx":73,"end_idx":81},{"text":"Express.js","type":"Skill","start_idx":82,"end_idx":92},{"text":"Web","type":"Skill","start_idx":93,"end_idx":96},{"text":"Servers","type":"Skill","start_idx":97,"end_idx":104},{"text":"Web","type":"Skill","start_idx":105,"end_idx":108},{"text":"Logic,","type":"Skill","start_idx":109,"end_idx":115},{"text":"Web","type":"Skill","start_idx":116,"end_idx":119},{"text":"Sphere,","type":"Skill","start_idx":120,"end_idx":127},{"text":"Apache","type":"Skill","start_idx":128,"end_idx":134},{"text":"Tomcat,","type":"Skill","start_idx":135,"end_idx":142},{"text":"JBOSS.","type":"Skill","start_idx":143,"end_idx":149}]},{"text":"Web Services SOAP, RESTful API, WSDL","entities":[{"text":"SOAP,","type":"Skill","start_idx":13,"end_idx":18},{"text":"RESTful","type":"Skill","start_idx":19,"end_idx":26},{"text":"API,","type":"Skill","start_idx":27,"end_idx":31},{"text":"WSDL","type":"Skill","start_idx":32,"end_idx":36}]}]
