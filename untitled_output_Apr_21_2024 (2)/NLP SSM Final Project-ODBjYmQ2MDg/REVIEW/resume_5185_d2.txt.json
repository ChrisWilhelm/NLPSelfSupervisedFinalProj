[{"text":"Azure VM Admin, ETL Specialist Azure VM Admin, ETL Specialist Azure VM Admin, ETL Specialist - Analytikus Work Experience Azure VM Admin, ETL Specialist Analytikus - Miami, FL August 2015 to Present Created and administrated Azure virtual machine.","entities":[{"text":"Azure VM Admin,","type":"Job Title","start_idx":0,"end_idx":15},{"text":"ETL Specialist Azure VM Admin,","type":"Job Title","start_idx":16,"end_idx":46},{"text":"ETL Specialist Azure VM Admin,","type":"Job Title","start_idx":47,"end_idx":77},{"text":"ETL Specialist","type":"Job Title","start_idx":78,"end_idx":92},{"text":"Analytikus","type":"Company Name","start_idx":95,"end_idx":105},{"text":"Azure VM Admin, ETL Specialist","type":"Job Title","start_idx":122,"end_idx":152},{"text":"Analytikus","type":"Company Name","start_idx":153,"end_idx":163},{"text":"Miami,","type":"Location","start_idx":166,"end_idx":172},{"text":"FL","type":"Location","start_idx":173,"end_idx":175},{"text":"August 2015","type":"Date","start_idx":176,"end_idx":187},{"text":"Present","type":"Date","start_idx":191,"end_idx":198},{"text":"Azure virtual machine.","type":"Skill","start_idx":225,"end_idx":247}]},{"text":"Federated Active directory with Azure Active Directory (Office 365).","entities":[{"text":"Active directory","type":"Skill","start_idx":10,"end_idx":26},{"text":"Azure","type":"Skill","start_idx":32,"end_idx":37},{"text":"Azure Active Directory","type":"Skill","start_idx":32,"end_idx":54},{"text":"(Office 365).","type":"Skill","start_idx":55,"end_idx":68}]},{"text":"Provided password synchronization AD to Azure AD.","entities":[{"text":"AD","type":"Skill","start_idx":34,"end_idx":36},{"text":"Azure AD.","type":"Skill","start_idx":40,"end_idx":49}]},{"text":"Configured SSRS Portal to share reports between SQL server accounts.","entities":[{"text":"SSRS","type":"Skill","start_idx":11,"end_idx":15},{"text":"SQL server","type":"Skill","start_idx":48,"end_idx":58}]},{"text":"Provided database administration (Users and permissions).","entities":[{"text":"database administration","type":"Skill","start_idx":9,"end_idx":32}]},{"text":"Generated data encryption process.","entities":[]},{"text":"Configured Gateway and VPN in Azure VM.","entities":[{"text":"Gateway","type":"Skill","start_idx":11,"end_idx":18},{"text":"VPN","type":"Skill","start_idx":23,"end_idx":26},{"text":"Azure VM.","type":"Skill","start_idx":30,"end_idx":39}]},{"text":"Provided SharePoint domain configuration with Office 365.","entities":[{"text":"SharePoint","type":"Skill","start_idx":9,"end_idx":19},{"text":"Office 365.","type":"Skill","start_idx":46,"end_idx":57}]},{"text":"Designed and implemented SSIS processes.","entities":[{"text":"SSIS","type":"Skill","start_idx":25,"end_idx":29}]},{"text":"Designed SSAS cubes.","entities":[{"text":"SSAS cubes.","type":"Skill","start_idx":9,"end_idx":20}]},{"text":"Provided data modeling.","entities":[]},{"text":"Designed roles in SSAS to manage row-level-security using Active Directory.","entities":[{"text":"SSAS","type":"Skill","start_idx":18,"end_idx":22},{"text":"row-level-security","type":"Skill","start_idx":33,"end_idx":51},{"text":"Active Directory.","type":"Skill","start_idx":58,"end_idx":75}]},{"text":"Installed and configured PowerBI installation and PowerBI Gateways in Azure VM.","entities":[{"text":"PowerBI","type":"Skill","start_idx":25,"end_idx":32},{"text":"PowerBI Gateways","type":"Skill","start_idx":50,"end_idx":66},{"text":"Azure VM.","type":"Skill","start_idx":70,"end_idx":79}]},{"text":"Managed Power BI portal sharing dashboards between Office 365 accounts.","entities":[{"text":"Power BI","type":"Skill","start_idx":8,"end_idx":16},{"text":"Office 365","type":"Skill","start_idx":51,"end_idx":61}]},{"text":"Designed and implemented strategical dashboards in Power BI and SSRS.","entities":[{"text":"strategical dashboards","type":"Skill","start_idx":25,"end_idx":47},{"text":"Power BI","type":"Skill","start_idx":51,"end_idx":59},{"text":"SSRS.","type":"Skill","start_idx":64,"end_idx":69}]},{"text":"Manage PowerBI row-level-security Manage PowerBI service creating workspaces, sharing dashboards, scheduling datasets updates, and giving user permissions Created sheets and dashboards in Tableau.","entities":[{"text":"PowerBI","type":"Skill","start_idx":7,"end_idx":14},{"text":"PowerBI","type":"Skill","start_idx":41,"end_idx":48},{"text":"workspaces,","type":"Skill","start_idx":66,"end_idx":77},{"text":"sharing dashboards,","type":"Skill","start_idx":78,"end_idx":97},{"text":"scheduling","type":"Skill","start_idx":98,"end_idx":108},{"text":"scheduling datasets","type":"Skill","start_idx":98,"end_idx":117},{"text":"Tableau.","type":"Skill","start_idx":188,"end_idx":196}]},{"text":"Shared and published dashboard.","entities":[]},{"text":"Exported sheets graphics as PDF and images.","entities":[]},{"text":"Used databases and csv files as sources in Tableau.","entities":[{"text":"databases","type":"Skill","start_idx":5,"end_idx":14},{"text":"Tableau.","type":"Skill","start_idx":43,"end_idx":51}]},{"text":"Created KPIs using Tableau components to use in graphics.","entities":[{"text":"KPIs","type":"Skill","start_idx":8,"end_idx":12},{"text":"Tableau","type":"Skill","start_idx":19,"end_idx":26}]},{"text":"Used Tableau Dashboard formatting.","entities":[{"text":"Tableau Dashboard","type":"Skill","start_idx":5,"end_idx":22}]},{"text":"Used Tableau Dashboard designer for some devices as phone and tablet.","entities":[{"text":"Tableau Dashboard","type":"Skill","start_idx":5,"end_idx":22}]},{"text":"Created aggregated calculations in PowerBI using DAX programming.","entities":[{"text":"PowerBI","type":"Skill","start_idx":35,"end_idx":42},{"text":"DAX programming.","type":"Skill","start_idx":49,"end_idx":65}]},{"text":"I led teams of developers Created project timelines Reviewed legacy Informatica ETL to migrate to SSIS Informatica ETL processes migration to SSIS Created ETL processes in Azure Databricks Created Python scripts to extract transform and load data Configured Data Factory in Azure Created Data Factory process to run SSIS ETL processes in Azure Technologies and tools used: Azure Virtual machine, Office online services, SSIS, SSAS, SSRS, Power BI, Azure Active Directory, SharePoint, Office 365, Tableau Desktop, Tableau server, Azure Data Factory, Azure machine learning, Informatica PowerCenter 9.1 Front- end and ETL Developer Praxis - Mexico City, MX June 2014 to July 2015 Provided BRD files analysis and technical sheets for construction of the SSI system (extractions and transformations).","entities":[{"text":"SSIS Informatica ETL","type":"Skill","start_idx":98,"end_idx":118},{"text":"SSIS","type":"Skill","start_idx":142,"end_idx":146},{"text":"Azure Databricks","type":"Skill","start_idx":172,"end_idx":188},{"text":"Python scripts","type":"Skill","start_idx":197,"end_idx":211},{"text":"Configured Data Factory","type":"Skill","start_idx":247,"end_idx":270},{"text":"Azure Created Data Factory","type":"Skill","start_idx":274,"end_idx":300},{"text":"SSIS ETL","type":"Skill","start_idx":316,"end_idx":324},{"text":"Azure Technologies","type":"Skill","start_idx":338,"end_idx":356},{"text":"Azure Virtual machine,","type":"Skill","start_idx":373,"end_idx":395},{"text":"Office online services,","type":"Skill","start_idx":396,"end_idx":419},{"text":"SSIS,","type":"Skill","start_idx":420,"end_idx":425},{"text":"SSAS,","type":"Skill","start_idx":426,"end_idx":431},{"text":"SSRS,","type":"Skill","start_idx":432,"end_idx":437},{"text":"Power BI,","type":"Skill","start_idx":438,"end_idx":447},{"text":"Azure","type":"Skill","start_idx":448,"end_idx":453},{"text":"Active Directory,","type":"Skill","start_idx":454,"end_idx":471},{"text":"SharePoint,","type":"Skill","start_idx":472,"end_idx":483},{"text":"Office 365,","type":"Skill","start_idx":484,"end_idx":495},{"text":"Tableau Desktop,","type":"Skill","start_idx":496,"end_idx":512},{"text":"Tableau server,","type":"Skill","start_idx":513,"end_idx":528},{"text":"Azure Data Factory,","type":"Skill","start_idx":529,"end_idx":548},{"text":"Azure machine learning,","type":"Skill","start_idx":549,"end_idx":572},{"text":"Informatica PowerCenter 9.1 Front- end","type":"Skill","start_idx":573,"end_idx":611},{"text":"ETL Developer","type":"Job Title","start_idx":616,"end_idx":629},{"text":"ETL Developer Praxis","type":"Skill","start_idx":616,"end_idx":636},{"text":"Praxis","type":"Company Name","start_idx":630,"end_idx":636},{"text":"Mexico City,","type":"Location","start_idx":639,"end_idx":651},{"text":"MX","type":"Location","start_idx":652,"end_idx":654},{"text":"June 2014","type":"Date","start_idx":655,"end_idx":664},{"text":"July 2015","type":"Date","start_idx":668,"end_idx":677},{"text":"SSI system","type":"Skill","start_idx":751,"end_idx":761}]},{"text":"Provided database model analysis used in the DB2 database.","entities":[{"text":"DB2 database.","type":"Skill","start_idx":45,"end_idx":58}]},{"text":"Constructed queries in the DB2 database from Oracle legacy tables, temporary tables, work tables and check digits.","entities":[{"text":"DB2 database","type":"Skill","start_idx":27,"end_idx":39}]},{"text":"Designed and developed ETL processes for SSI legacy in Informatica.","entities":[{"text":"ETL","type":"Skill","start_idx":23,"end_idx":26},{"text":"SSI legacy","type":"Skill","start_idx":41,"end_idx":51}]},{"text":"Programmed validation routines as mentioned in technical sheet.","entities":[]},{"text":"Programmed extraction queries to expedite migration Oracle database to DB2.","entities":[{"text":"Programmed","type":"Skill","start_idx":0,"end_idx":10},{"text":"Oracle database","type":"Skill","start_idx":52,"end_idx":67}]},{"text":"Programmed process to generate the current age for each user from the RFC.","entities":[{"text":"Programmed","type":"Skill","start_idx":0,"end_idx":10},{"text":"RFC.","type":"Skill","start_idx":70,"end_idx":74}]},{"text":"Programmed processes to generate check digits in the extraction of SSI system.","entities":[{"text":"Programmed","type":"Skill","start_idx":0,"end_idx":10},{"text":"SSI system.","type":"Skill","start_idx":67,"end_idx":78}]},{"text":"Provided application process for validating previous month and current month in SSI transformation.","entities":[{"text":"SSI transformation.","type":"Skill","start_idx":80,"end_idx":99}]},{"text":"Configured sessions in Informatica to execute the maps.","entities":[]},{"text":"Created cubes of some tables.","entities":[]},{"text":"Built OLAP cubes to optimize queries using dimensions.","entities":[]},{"text":"Deployed multidimensional DB.","entities":[]},{"text":"Configuration parameter files used in the execution for SSI sessions.","entities":[]},{"text":"Validated of ETL processes with BI specialist.","entities":[{"text":"ETL processes","type":"Skill","start_idx":13,"end_idx":26},{"text":"BI specialist.","type":"Skill","start_idx":32,"end_idx":46}]},{"text":"Validated output files with business specialist.","entities":[]},{"text":"Ran processes by shell in development and pre-production environments.","entities":[]},{"text":"Documentation: Technical design, functional design, Data model, test, Runbook Technologies and tools used: Secure Shell, shell script, FileZilla 3.5.3, Linux, SSIS, Informatica PowerCenter 9.1, Oracle 11g, DB2 ETL and Front- end Programmer Comisin Nacional de Seguridad - Mexico City, MX June 2013 to January 2014 Activities: Designed and developed ETL processes for data migration from SQL Server to Oracle, considering tables, indexes, store procedures, users, etc. Initial loaded to database Oracle using the dump process.","entities":[{"text":"Technical design,","type":"Skill","start_idx":15,"end_idx":32},{"text":"functional design,","type":"Skill","start_idx":33,"end_idx":51},{"text":"Data model,","type":"Skill","start_idx":52,"end_idx":63},{"text":"test,","type":"Skill","start_idx":64,"end_idx":69},{"text":"Secure Shell,","type":"Skill","start_idx":107,"end_idx":120},{"text":"shell script,","type":"Skill","start_idx":121,"end_idx":134},{"text":"FileZilla 3.5.3,","type":"Skill","start_idx":135,"end_idx":151},{"text":"Linux,","type":"Skill","start_idx":152,"end_idx":158},{"text":"SSIS,","type":"Skill","start_idx":159,"end_idx":164},{"text":"Informatica PowerCenter 9.1,","type":"Skill","start_idx":165,"end_idx":193},{"text":"Oracle 11g,","type":"Skill","start_idx":194,"end_idx":205},{"text":"DB2","type":"Skill","start_idx":206,"end_idx":209},{"text":"ETL and Front- end Programmer","type":"Job Title","start_idx":210,"end_idx":239},{"text":"Comisin Nacional de Seguridad","type":"Company Name","start_idx":240,"end_idx":269},{"text":"Mexico City,","type":"Location","start_idx":272,"end_idx":284},{"text":"MX","type":"Location","start_idx":285,"end_idx":287},{"text":"June 2013","type":"Date","start_idx":288,"end_idx":297},{"text":"January 2014","type":"Date","start_idx":301,"end_idx":313},{"text":"ETL processes","type":"Skill","start_idx":349,"end_idx":362},{"text":"data migration","type":"Skill","start_idx":367,"end_idx":381},{"text":"SQL Server","type":"Skill","start_idx":387,"end_idx":397},{"text":"Oracle,","type":"Skill","start_idx":401,"end_idx":408},{"text":"tables,","type":"Skill","start_idx":421,"end_idx":428},{"text":"indexes,","type":"Skill","start_idx":429,"end_idx":437},{"text":"store procedures,","type":"Skill","start_idx":438,"end_idx":455},{"text":"users,","type":"Skill","start_idx":456,"end_idx":462}]},{"text":"Construction of queries for extraction using table joins, sources that were delegated.","entities":[{"text":"table joins,","type":"Skill","start_idx":45,"end_idx":57}]},{"text":"Construction of XML files for uploading information through a system of extraction and load to a repository.","entities":[{"text":"XML files","type":"Skill","start_idx":16,"end_idx":25}]},{"text":"Analysis of the structure of the information to develop the process of deltas of Oracle and SQL Server sources.","entities":[{"text":"Oracle","type":"Skill","start_idx":81,"end_idx":87},{"text":"SQL Server","type":"Skill","start_idx":92,"end_idx":102}]},{"text":"Documented processes.","entities":[]},{"text":"Programmed process to generate reports in SSRS.","entities":[{"text":"SSRS.","type":"Skill","start_idx":42,"end_idx":47}]},{"text":"Added a dataset report based on queries.","entities":[]},{"text":"Did PowerPivot data analysis.","entities":[{"text":"PowerPivot","type":"Skill","start_idx":4,"end_idx":14}]},{"text":"Technologies and tools used: IBM Data Stage InfoSphere, SQL Server 2008, Secure Shell, Shell Script, Linux, Oracle 11g, SQL Server Reporting Services (SSRS), SQL Server Management Studio 2008, PowerPivot ETL Programmer Servicio de Administracion Tributaria, SAT - Mexico City, MX March 2013 to June 2013 Reviewed the process of \"large taxpayers\" in SAT for issues like NEPE, ANNUAL, DYP, and DIME.","entities":[{"text":"IBM Data Stage InfoSphere,","type":"Skill","start_idx":29,"end_idx":55},{"text":"SQL Server 2008,","type":"Skill","start_idx":56,"end_idx":72},{"text":"Secure Shell,","type":"Skill","start_idx":73,"end_idx":86},{"text":"Shell Script,","type":"Skill","start_idx":87,"end_idx":100},{"text":"Linux,","type":"Skill","start_idx":101,"end_idx":107},{"text":"Oracle 11g,","type":"Skill","start_idx":108,"end_idx":119},{"text":"SQL Server Reporting Services (SSRS),","type":"Skill","start_idx":120,"end_idx":157},{"text":"SQL Server Management Studio 2008,","type":"Skill","start_idx":158,"end_idx":192},{"text":"PowerPivot","type":"Skill","start_idx":193,"end_idx":203},{"text":"ETL Programmer","type":"Job Title","start_idx":204,"end_idx":218},{"text":"Servicio de Administracion Tributaria, SAT","type":"Company Name","start_idx":219,"end_idx":261},{"text":"Mexico City,","type":"Location","start_idx":264,"end_idx":276},{"text":"MX","type":"Location","start_idx":277,"end_idx":279},{"text":"March 2013","type":"Date","start_idx":280,"end_idx":290},{"text":"June 2013","type":"Date","start_idx":294,"end_idx":303}]},{"text":"Constructed routines in the process control tablet speeding up the step, due to the time frame allowed the company.","entities":[]},{"text":"Managed VOID analysis process.","entities":[]},{"text":"Provided process analysis of loading tables NEPE, ANUALES DIME, and DYP issues.","entities":[]},{"text":"Addressed parameters directly in the project environment variables, also the input and output files, all in accordance with the standards in the business handled.","entities":[]},{"text":"Changed the comparison process with new RFC.","entities":[]},{"text":"Created bar charts in SQL Server Reporting Services (SSRS).","entities":[{"text":"SQL Server Reporting Services (SSRS).","type":"Skill","start_idx":22,"end_idx":59}]},{"text":"Created dashboards of correlated column charts.","entities":[{"text":"dashboards","type":"Skill","start_idx":8,"end_idx":18},{"text":"column charts.","type":"Skill","start_idx":33,"end_idx":47}]},{"text":"Created dashboards in Power BI using Power Query.","entities":[{"text":"Power BI","type":"Skill","start_idx":22,"end_idx":30},{"text":"Power Query.","type":"Skill","start_idx":37,"end_idx":49}]},{"text":"Created dashboards of nested calculations with Power Query.","entities":[{"text":"Power Query.","type":"Skill","start_idx":47,"end_idx":59}]},{"text":"Technologies and tools used: IBM Data Stage InfoSphere, Power BI, SSRS, Informix, Linux, Shell script, FileZilla, SSH ETL Programmer Archysoft - Mexico City, MX May 2012 to February 2013 Reviewed and processed scheduling information extraction data sources for DB2 ACCEDER and SINDO systems.","entities":[{"text":"IBM Data Stage InfoSphere,","type":"Skill","start_idx":29,"end_idx":55},{"text":"Power BI,","type":"Skill","start_idx":56,"end_idx":65},{"text":"SSRS,","type":"Skill","start_idx":66,"end_idx":71},{"text":"Informix,","type":"Skill","start_idx":72,"end_idx":81},{"text":"Linux,","type":"Skill","start_idx":82,"end_idx":88},{"text":"Shell script,","type":"Skill","start_idx":89,"end_idx":102},{"text":"FileZilla,","type":"Skill","start_idx":103,"end_idx":113},{"text":"SSH","type":"Skill","start_idx":114,"end_idx":117},{"text":"ETL Programmer","type":"Job Title","start_idx":118,"end_idx":132},{"text":"Archysoft","type":"Company Name","start_idx":133,"end_idx":142},{"text":"Mexico City,","type":"Location","start_idx":145,"end_idx":157},{"text":"MX","type":"Location","start_idx":158,"end_idx":160},{"text":"May 2012","type":"Date","start_idx":161,"end_idx":169},{"text":"February 2013","type":"Date","start_idx":173,"end_idx":186},{"text":"DB2 ACCEDER","type":"Skill","start_idx":261,"end_idx":272},{"text":"SINDO systems.","type":"Skill","start_idx":277,"end_idx":291}]},{"text":"Programmed 40 ETL jobs for data cleansing and over 25 functions were developed to implement the business rules defined by the user area.","entities":[{"text":"ETL jobs","type":"Skill","start_idx":14,"end_idx":22},{"text":"data cleansing","type":"Skill","start_idx":27,"end_idx":41}]},{"text":"For temporary and permanent storage of information, programmed SQL scripts for creating tables, indexes, and other elements that allows for holding and processing information.","entities":[{"text":"SQL scripts","type":"Skill","start_idx":63,"end_idx":74}]},{"text":"Performed unit testing and was integral to all tests constructed on Data Stage jobs as well as job functions Q Basic and sequences built by work team.","entities":[{"text":"Q Basic","type":"Skill","start_idx":109,"end_idx":116}]},{"text":"The scheduled ETL processes were performed to standardize and clean the information had not suitable quality for the purposes needed by the user.","entities":[{"text":"ETL processes","type":"Skill","start_idx":14,"end_idx":27}]},{"text":"Developed a process to eliminate duplicate records, extracting flow records that were repeated.","entities":[]},{"text":"This was performed by using hash files and other tables directly from SQL.","entities":[{"text":"SQL.","type":"Skill","start_idx":70,"end_idx":74}]},{"text":"Programmed before and after functions to make validation digits, dates processing and consistency of information uploaded to the data warehouse IMSS systems.","entities":[]},{"text":"Designed and programmed loading processes information and data cleansing for data marts of SINDO and ACCEDER the two main systems of IMSS.","entities":[]},{"text":"Create Extract/Load detail dashboard in SSIS Technologies and tools used: SQL Server Management Studio, SQL Server 2012, Microsoft Visual Studio, DB2, Linux, Shell, SQL Developer, IBM Data Studio, SQL Developer, FileZilla, SSH, SSAS, SSIS IBM DataStage Developer Arquitectura en Software Consulting Firm - Mexico City, MX October 2011 to April 2012 at IMSS (Mexican Social Security Institute) Data Analysis Provided data analysis, identification of special characters, and structure definitions files.","entities":[{"text":"SQL Server Management Studio,","type":"Skill","start_idx":74,"end_idx":103},{"text":"SQL Server 2012,","type":"Skill","start_idx":104,"end_idx":120},{"text":"Microsoft Visual Studio,","type":"Skill","start_idx":121,"end_idx":145},{"text":"DB2,","type":"Skill","start_idx":146,"end_idx":150},{"text":"Linux,","type":"Skill","start_idx":151,"end_idx":157},{"text":"Shell,","type":"Skill","start_idx":158,"end_idx":164},{"text":"SQL Developer,","type":"Skill","start_idx":165,"end_idx":179},{"text":"IBM Data Studio,","type":"Skill","start_idx":180,"end_idx":196},{"text":"SQL Developer,","type":"Skill","start_idx":197,"end_idx":211},{"text":"FileZilla,","type":"Skill","start_idx":212,"end_idx":222},{"text":"SSH,","type":"Skill","start_idx":223,"end_idx":227},{"text":"SSAS,","type":"Skill","start_idx":228,"end_idx":233},{"text":"SSIS","type":"Skill","start_idx":234,"end_idx":238},{"text":"IBM DataStage Developer","type":"Job Title","start_idx":239,"end_idx":262},{"text":"Arquitectura en Software Consulting Firm","type":"Company Name","start_idx":263,"end_idx":303},{"text":"Mexico City,","type":"Location","start_idx":306,"end_idx":318},{"text":"MX","type":"Location","start_idx":319,"end_idx":321},{"text":"October 2011","type":"Date","start_idx":322,"end_idx":334},{"text":"April 2012","type":"Date","start_idx":338,"end_idx":348},{"text":"Data Analysis","type":"Skill","start_idx":393,"end_idx":406}]},{"text":"Prepared reports of frequencies for defining business rules.","entities":[]},{"text":"Documented processes and information flows.","entities":[]},{"text":"Developed notes profiling tables analyzed.","entities":[]},{"text":"Identified anomalies data field and source of information.","entities":[]},{"text":"Data Cleaning Programmed and processed shell script (UNIX) for cleaning planes, identification and correction of special characters and inconsistent data files.","entities":[{"text":"Data Cleaning","type":"Skill","start_idx":0,"end_idx":13},{"text":"shell script","type":"Skill","start_idx":39,"end_idx":51},{"text":"(UNIX)","type":"Skill","start_idx":52,"end_idx":58}]},{"text":"Designed and programmed processes for IBM DataStage, business rules application for standardization of information.","entities":[{"text":"IBM DataStage,","type":"Skill","start_idx":38,"end_idx":52}]},{"text":"Information Extraction Analyzed and designed programming process for the extraction of information contained in data sources DB2 and ORACLE 11 g. Provided SQL programming scripts to build tables, indexes and constraints.","entities":[{"text":"DB2","type":"Skill","start_idx":125,"end_idx":128},{"text":"ORACLE 11 g.","type":"Skill","start_idx":133,"end_idx":145},{"text":"Provided SQL","type":"Skill","start_idx":146,"end_idx":158}]},{"text":"Data Loading Scheduled ETL (DataStage) processes for the initial loads databases cleaning and standardization information (DB2).","entities":[{"text":"Data Loading","type":"Skill","start_idx":0,"end_idx":12},{"text":"ETL (DataStage)","type":"Skill","start_idx":23,"end_idx":38},{"text":"(DB2).","type":"Skill","start_idx":122,"end_idx":128}]},{"text":"Developed ETL processes using DataStage to load information to the analysis database (DB2) Technologies and tools used: IBM DataStage InfoSphere, Oracle 10G y 11g, DB2 9.5 y 97, Linux, Shell, SQL, Developer, IBM Data Studio, SQL Server, FileZilla, SSH Education Engineering Information and Communication Technologies Universidad Tecnolgica del valle de Toluca","entities":[{"text":"DataStage","type":"Skill","start_idx":30,"end_idx":39},{"text":"(DB2)","type":"Skill","start_idx":85,"end_idx":90},{"text":"IBM DataStage InfoSphere,","type":"Skill","start_idx":120,"end_idx":145},{"text":"Oracle 10G y 11g,","type":"Skill","start_idx":146,"end_idx":163},{"text":"DB2 9.5 y 97,","type":"Skill","start_idx":164,"end_idx":177},{"text":"Linux,","type":"Skill","start_idx":178,"end_idx":184},{"text":"Shell,","type":"Skill","start_idx":185,"end_idx":191},{"text":"SQL,","type":"Skill","start_idx":192,"end_idx":196},{"text":"Developer,","type":"Skill","start_idx":197,"end_idx":207},{"text":"IBM Data Studio,","type":"Skill","start_idx":208,"end_idx":224},{"text":"SQL Server,","type":"Skill","start_idx":225,"end_idx":236},{"text":"FileZilla,","type":"Skill","start_idx":237,"end_idx":247},{"text":"SSH","type":"Skill","start_idx":248,"end_idx":251},{"text":"Engineering Information and Communication Technologies","type":"Degree","start_idx":262,"end_idx":316},{"text":"Universidad Tecnolgica del valle de Toluca","type":"College","start_idx":317,"end_idx":359}]}]
